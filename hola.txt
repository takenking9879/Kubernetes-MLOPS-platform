2026-01-26 16:00:58.688 | 2026-01-26 14:00:58,687	INFO cli.py:41 -- Job submission server address: http://kuberay-job-gmqdj-head-svc.ray.svc.cluster.local:8265
2026-01-26 16:00:59.036 | 2026-01-26 14:00:59,036	SUCC cli.py:65 -- ----------------------------------------------
2026-01-26 16:00:59.036 | 2026-01-26 14:00:59,036	SUCC cli.py:66 -- Job 'kuberay-job-hw8k8' submitted successfully
2026-01-26 16:00:59.036 | 2026-01-26 14:00:59,036	SUCC cli.py:67 -- ----------------------------------------------
2026-01-26 16:00:59.036 | 2026-01-26 14:00:59,036	INFO cli.py:291 -- Next steps
2026-01-26 16:00:59.036 | 2026-01-26 14:00:59,036	INFO cli.py:292 -- Query the logs of the job:
2026-01-26 16:00:59.036 | 2026-01-26 14:00:59,036	INFO cli.py:294 -- ray job logs kuberay-job-hw8k8
2026-01-26 16:00:59.036 | 2026-01-26 14:00:59,036	INFO cli.py:296 -- Query the status of the job:
2026-01-26 16:00:59.036 | 2026-01-26 14:00:59,036	INFO cli.py:298 -- ray job status kuberay-job-hw8k8
2026-01-26 16:00:59.036 | 2026-01-26 14:00:59,036	INFO cli.py:300 -- Request the job to be stopped:
2026-01-26 16:00:59.037 | 2026-01-26 14:00:59,036	INFO cli.py:302 -- ray job stop kuberay-job-hw8k8
2026-01-26 16:01:00.658 | 2026-01-26 14:01:00,658	INFO cli.py:41 -- Job submission server address: http://kuberay-job-gmqdj-head-svc.ray.svc.cluster.local:8265
2026-01-26 16:01:02.675 | 2026-01-26 14:00:58,763	INFO job_manager.py:568 -- Runtime env is setting up.
2026-01-26 16:01:02.675 | Running entrypoint for job kuberay-job-hw8k8: python3 /home/ray/app/repo/k3s/kuberay/main.py
2026-01-26 16:01:05.682 | 2026-01-26 14:01:04,017 - KubeRayTraining - DEBUG - Parameters retrieved from /home/ray/app/repo/k3s/params.yaml
2026-01-26 16:01:06.683 | 2026-01-26 14:01:05,353 - KubeRayTraining - INFO - [RAY CLUSTER RESOURCES]
2026-01-26 16:01:06.683 |             â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2026-01-26 16:01:06.683 |             CPU           : 0.0 / 18.0
2026-01-26 16:01:06.683 |             Memory        : 0B / 13.00GiB
2026-01-26 16:01:06.683 |             Object Store  : 0B / 8.23GiB
2026-01-26 16:01:06.683 |             â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2026-01-26 16:01:07.684 | 2026-01-26 14:01:05,761 - KubeRayTraining - INFO - Minio connection verified. Buckets: ['frontend-crm-bucket', 'k8s-mlops-platform-bucket']
2026-01-26 16:01:07.684 | 2026-01-26 14:01:06,900	INFO worker.py:1696 -- Using address 10.1.5.75:6379 set in the environment variable RAY_ADDRESS
2026-01-26 16:01:07.684 | 2026-01-26 14:01:06,902	INFO worker.py:1837 -- Connecting to existing Ray cluster at address: 10.1.5.75:6379...
2026-01-26 16:01:07.684 | 2026-01-26 14:01:06,915	INFO worker.py:2014 -- Connected to Ray cluster. View the dashboard at http://10.1.5.75:8265 
2026-01-26 16:01:07.684 | /home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py:2062: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0
2026-01-26 16:01:07.684 |   warnings.warn(
2026-01-26 16:01:07.684 | 
2026-01-26 16:01:07.684 |   0%|          | 0.00/1.00 [00:00<?, ? file/s]
2026-01-26 16:01:07.684 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:00<?, ? file/s]
2026-01-26 16:01:07.684 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:00<?, ? file/s]
2026-01-26 16:01:07.684 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:00<?, ? file/s]
2026-01-26 16:01:08.684 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:00<?, ? file/s]
2026-01-26 16:01:08.684 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:00<?, ? file/s]
2026-01-26 16:01:08.684 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:00<?, ? file/s]
2026-01-26 16:01:08.684 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:00<?, ? file/s]
2026-01-26 16:01:08.684 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:00<?, ? file/s]
2026-01-26 16:01:08.684 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:00<?, ? file/s]
2026-01-26 16:01:08.684 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:00<?, ? file/s]
2026-01-26 16:01:08.684 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:01<?, ? file/s]
2026-01-26 16:01:08.684 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:01<?, ? file/s]
2026-01-26 16:01:08.684 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:01<?, ? file/s]
2026-01-26 16:01:08.684 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:01<?, ? file/s]
2026-01-26 16:01:08.684 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:01<?, ? file/s]
2026-01-26 16:01:08.684 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:01<?, ? file/s]
2026-01-26 16:01:08.684 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:01<?, ? file/s]
2026-01-26 16:01:09.686 | Parquet dataset sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.00/1.00 [00:01<00:00, 1.68s/ file]
2026-01-26 16:01:09.686 | 2026-01-26 14:01:08,634	INFO parquet_datasource.py:728 -- Estimated parquet encoding ratio is 2.660.
2026-01-26 16:01:09.686 | 2026-01-26 14:01:08,635	INFO parquet_datasource.py:788 -- Estimated parquet reader batch size at 1220162 rows
2026-01-26 16:01:11.690 | 2026-01-26 14:01:09,779 - KubeRayTraining - INFO - Data loaded from s3://k8s-mlops-platform-bucket/v1/processed/train.
2026-01-26 16:01:11.690 | 
2026-01-26 16:01:11.690 |   0%|          | 0.00/1.00 [00:00<?, ? file/s]
2026-01-26 16:01:11.690 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:00<?, ? file/s]
2026-01-26 16:01:11.690 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:00<?, ? file/s]
2026-01-26 16:01:11.690 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:00<?, ? file/s]
2026-01-26 16:01:11.690 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:00<?, ? file/s]
2026-01-26 16:01:11.690 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:00<?, ? file/s]
2026-01-26 16:01:11.690 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:00<?, ? file/s]
2026-01-26 16:01:11.690 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:00<?, ? file/s]
2026-01-26 16:01:12.691 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:00<?, ? file/s]
2026-01-26 16:01:12.691 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:00<?, ? file/s]
2026-01-26 16:01:12.691 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:00<?, ? file/s]
2026-01-26 16:01:12.691 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:01<?, ? file/s]
2026-01-26 16:01:12.691 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:01<?, ? file/s]
2026-01-26 16:01:12.691 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:01<?, ? file/s]
2026-01-26 16:01:12.691 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:01<?, ? file/s]
2026-01-26 16:01:12.691 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:01<?, ? file/s]
2026-01-26 16:01:12.691 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:01<?, ? file/s]
2026-01-26 16:01:12.691 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:01<?, ? file/s]
2026-01-26 16:01:12.691 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:01<?, ? file/s]
2026-01-26 16:01:12.691 | Parquet dataset sampling:   0%|          | 0.00/1.00 [00:01<?, ? file/s]
2026-01-26 16:01:12.691 | Parquet dataset sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.00/1.00 [00:01<00:00, 1.84s/ file]
2026-01-26 16:01:13.693 | 2026-01-26 14:01:12,500	INFO parquet_datasource.py:728 -- Estimated parquet encoding ratio is 2.783.
2026-01-26 16:01:13.693 | 2026-01-26 14:01:12,500	INFO parquet_datasource.py:788 -- Estimated parquet reader batch size at 1220162 rows
2026-01-26 16:01:13.693 | 2026-01-26 14:01:12,506 - KubeRayTraining - INFO - Data loaded from s3://k8s-mlops-platform-bucket/v1/processed/val.
2026-01-26 16:01:13.693 | 2026-01-26 14:01:12,506 - KubeRayTraining - INFO - Starting training using framework: xgboost
2026-01-26 16:01:18.699 | (TrainController pid=1364) Attempting to start training worker group of size 2 with the following resources: [{'CPU': 8}] * 2
2026-01-26 16:01:21.705 | (TrainController pid=1364) Started training worker group of size 2: 
2026-01-26 16:01:21.705 | (TrainController pid=1364) - (ip=10.1.5.77, pid=501) world_rank=0, local_rank=0, node_rank=0
2026-01-26 16:01:21.705 | (TrainController pid=1364) - (ip=10.1.5.76, pid=438) world_rank=1, local_rank=0, node_rank=1
2026-01-26 16:01:22.705 | (RayTrainWorker pid=438, ip=10.1.5.76) [14:01:20] Task [xgboost.ray-rank=00000001]:59e6cd77c34159a1739982fe02000000 got rank 1
2026-01-26 16:01:22.705 | (RayTrainWorker pid=501, ip=10.1.5.77) [xgboost] Worker using nthread=8 | OMP_NUM_THREADS=8
2026-01-26 16:01:22.705 | (SplitCoordinator pid=1536) Registered dataset logger for dataset train_2_0
2026-01-26 16:01:22.705 | (SplitCoordinator pid=1536) Starting execution of Dataset train_2_0. Full logs are in /tmp/ray/session_2026-01-26_14-00-26_889640_1/logs/ray-data
2026-01-26 16:01:22.705 | (SplitCoordinator pid=1536) Execution plan of Dataset train_2_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> OutputSplitter[split(2, equal=True)]
2026-01-26 16:01:22.705 | (raylet) WARNING: 4 PYTHON worker processes have been started on node: d6c489ac03fced80a82a7fce8fc8b90c6955b000cbceb79cbe746d26 with address: 10.1.5.75. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).
2026-01-26 16:01:22.705 | (SplitCoordinator pid=1536)  âœ”ï¸  Dataset train_2_0 execution finished in 1.05 seconds 10â€¦ 2k/â€¦   1.95 k rowâ€¦ 
2026-01-26 16:01:22.705 | (SplitCoordinator pid=1536)   â”‚ Active/total resources: Active & requested resources: 0/2 CPU, 47.1KiB/4.1Gâ€¦
2026-01-26 16:01:22.705 | (SplitCoordinator pid=1536)   â”‚                                                                             
2026-01-26 16:01:22.705 | (SplitCoordinator pid=1536)   â”œâ”€   ReadParquet->SplitBlocks(36) 100% â”â”â”â”â”â”â” 2k/2k [ 0:00:â€¦ , 1.97 k row/s ]
2026-01-26 16:01:22.705 | (SplitCoordinator pid=1536)   â”‚    0.0 CPU, 0.0B object store; Tasks: 0; Queued blocks: 0;                  
2026-01-26 16:01:22.705 | (SplitCoordinator pid=1536)   â”œâ”€   split(2, equal=True) 100% â”â”â”â”â”â”â”â”â”â” 2k/2k [ 0:00:01 , 1.97 k row/s ]    
2026-01-26 16:01:22.705 | (SplitCoordinator pid=1536)   â”‚    0.0 CPU, 47.1KiB object store; Tasks: 0: [0/33 objects local]; Queued blâ€¦
2026-01-26 16:01:22.705 | (SplitCoordinator pid=1536)                                                                                 âœ”ï¸  Dataset train_2_0 execution finished in 1.05 seconds
2026-01-26 16:01:22.705 | (RayTrainWorker pid=438, ip=10.1.5.76)  âœ”ï¸  Dataset dataset_7_0 execution finished in 0.00 seconds 10â€¦ 1k/â€¦ [  ? rows/s 
2026-01-26 16:01:23.707 | (RayTrainWorker pid=438, ip=10.1.5.76)   â”‚ Active/total resources: Active & requested resources: 0/2 CPU, 0.0B/4.1GiB â€¦
2026-01-26 16:01:23.707 | (RayTrainWorker pid=501, ip=10.1.5.77) Exiting prefetcher's background thread
2026-01-26 16:01:26.711 | (SplitCoordinator pid=1594)  âœ”ï¸  Dataset val_4_0 execution finished in 1.91 seconds 10â€¦ 3k/â€¦ [  1.59 k row/s 
2026-01-26 16:01:26.711 | (RayTrainWorker pid=438, ip=10.1.5.76)  âœ”ï¸  Dataset dataset_8_0 execution finished in 0.00 seconds 10â€¦ 1.50k/â€¦   ? rowâ€¦ 
2026-01-26 16:01:26.711 | (RayTrainWorker pid=438, ip=10.1.5.76) Reporting training result 1: TrainingReport(checkpoint=None, metrics={'validation-mlogloss': 0.20996513929466407, 'validation-merror': 0.00466666666666667}, validation_spec=None)
2026-01-26 16:01:26.711 | (RayTrainWorker pid=438, ip=10.1.5.76) Reporting training result 3: TrainingReport(checkpoint=None, metrics={}, validation_spec=None)
2026-01-26 16:01:27.712 | (RayTrainWorker pid=501, ip=10.1.5.77) Checkpoint successfully created at: Checkpoint(filesystem=s3, path=k8s-mlops-platform-bucket/v1/models/xgboost/checkpoint_2026-01-26_14-01-24.667914)
2026-01-26 16:01:27.712 | (RayTrainWorker pid=501, ip=10.1.5.77) Reporting training result 3: TrainingReport(checkpoint=Checkpoint(filesystem=s3, path=k8s-mlops-platform-bucket/v1/models/xgboost/checkpoint_2026-01-26_14-01-24.667914), metrics={}, validation_spec=None)
2026-01-26 16:01:27.712 | (RayTrainWorker pid=501, ip=10.1.5.77) [14:01:20] Task [xgboost.ray-rank=00000000]:268cbf434fa4c2d28ab7540c02000000 got rank 0
2026-01-26 16:01:27.712 | (RayTrainWorker pid=501, ip=10.1.5.77) Registered dataset logger for dataset dataset_9_0 [repeated 5x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)
2026-01-26 16:01:27.712 | (SplitCoordinator pid=1594) Starting execution of Dataset val_4_0. Full logs are in /tmp/ray/session_2026-01-26_14-00-26_889640_1/logs/ray-data
2026-01-26 16:01:27.712 | (SplitCoordinator pid=1594) Execution plan of Dataset val_4_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> OutputSplitter[split(2, equal=True)]
2026-01-26 16:01:28.853 | (RayTrainWorker pid=438, ip=10.1.5.76) [xgboost] Worker train_time_sec=2.64
2026-01-26 16:01:33.862 | 2026-01-26 14:01:32,767	INFO logging.py:397 -- Registered dataset logger for dataset dataset_10_0
2026-01-26 16:01:33.862 | 2026-01-26 14:01:32,780	INFO streaming_executor.py:174 -- Starting execution of Dataset dataset_10_0. Full logs are in /tmp/ray/session_2026-01-26_14-00-26_889640_1/logs/ray-data
2026-01-26 16:01:33.862 | 2026-01-26 14:01:32,780	INFO streaming_executor.py:175 -- Execution plan of Dataset dataset_10_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> TaskPoolMapOperator[MapBatches(predict_and_cm_batch)]
2026-01-26 16:01:40.878 | (autoscaler +38s) Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.
2026-01-26 16:01:40.878 | (autoscaler +38s) No available node types can fulfill cluster constraint: {'CPU': 1.0}*22. Add suitable node types to this cluster to resolve this issue.
2026-01-26 16:01:40.878 |  âœ”ï¸  Dataset dataset_10_0 execution finished in 7.97 seconds 10â€¦ 36/â€¦   4.53 roâ€¦ 
2026-01-26 16:01:40.878 |   â”‚ Active/total resources: Active & requested resources: 0/18 CPU, 1.5KiB/4.1Gâ€¦
2026-01-26 16:01:40.878 |   â”‚                                                                             
2026-01-26 16:01:40.878 |   â”œâ”€   ReadParquet->SplitBlocks(36) 100% â”â”â”â”â”â”â” 3k/3k [ 0:00:â€¦ , 809.00 row/s ]
2026-01-26 16:01:40.878 |   â”‚    0.0 CPU, 0.0B object store; Tasks: 0; Queued blocks: 0;                  
2026-01-26 16:01:40.878 |   â”œâ”€   MapBatches(predict_and_cm_batch) 100% â”â”â”â”â” 36/36 [ 0:00:â€¦ , 4.53 row/s ]
2026-01-26 16:01:40.878 |   â”‚    0.0 CPU, 1.5KiB object store; Tasks: 0; Queued blocks: 0;                
2026-01-26 16:01:40.878 |                                                                                 2026-01-26 14:01:40,780	INFO progress_manager.py:347 -- âœ”ï¸  Dataset dataset_10_0 execution finished in 7.97 seconds
2026-01-26 16:01:40.878 | 2026-01-26 14:01:40,782	INFO util.py:257 -- Exiting prefetcher's background thread
2026-01-26 16:01:40.878 | 2026-01-26 14:01:40,784 - KubeRayTraining - INFO - Training completed successfully.
2026-01-26 16:01:41.879 | 2026-01-26 14:01:40,784 - KubeRayTraining - INFO - Exportando modelo final de xgboost a S3...
2026-01-26 16:01:44.883 | 2026-01-26 14:01:42,921 - KubeRayTraining - INFO - Modelo guardado exitosamente en: s3://k8s-mlops-platform-bucket/v1/models/model_xgboost.pkl
2026-01-26 16:01:44.883 | 2026-01-26 14:01:42,921 - KubeRayTraining - INFO - xgboost multiclass metrics time = 8.88 s
2026-01-26 16:01:44.883 | ğŸƒ View run xgboost_final_xgboost at: http://my-mlflow/#/experiments/1/runs/20efc0dd1f204beab55737b2e6069e1a
2026-01-26 16:01:44.883 | ğŸ§ª View experiment at: http://my-mlflow/#/experiments/1
2026-01-26 16:01:44.883 | (SplitCoordinator pid=1594)   â”‚ Active/total resources: Active & requested resources: 0/2 CPU, 53.3KiB/4.1Gâ€¦
2026-01-26 16:01:44.883 | (RayTrainWorker pid=501, ip=10.1.5.77)   â”‚                                                                              [repeated 5x across cluster]
2026-01-26 16:01:44.883 | (SplitCoordinator pid=1594)   â”œâ”€   ReadParquet->SplitBlocks(36) 100% â”â”â”â”â”â”â” 3k/3k [ 0:00:â€¦ , 1.60 k row/s ]
2026-01-26 16:01:44.883 | (SplitCoordinator pid=1594)   â”‚    0.0 CPU, 0.0B object store; Tasks: 0; Queued blocks: 0;                  
2026-01-26 16:01:44.883 | (SplitCoordinator pid=1594)   â”œâ”€   split(2, equal=True) 100% â”â”â”â”â”â”â”â”â”â” 3k/3k [ 0:00:01 , 1.60 k row/s ]    
2026-01-26 16:01:44.883 | (SplitCoordinator pid=1594)   â”‚    0.0 CPU, 53.3KiB object store; Tasks: 0: [0/33 objects local]; Queued blâ€¦
2026-01-26 16:01:44.883 | (RayTrainWorker pid=501, ip=10.1.5.77)                                                                                 âœ”ï¸  Dataset dataset_9_0 execution finished in 0.00 seconds [repeated 5x across cluster]
2026-01-26 16:01:44.883 | (RayTrainWorker pid=501, ip=10.1.5.77)  âœ”ï¸  Dataset dataset_6_0 execution finished in 0.00 seconds 10â€¦ 1k/â€¦ [  ? rows/s 
2026-01-26 16:01:44.883 | (RayTrainWorker pid=501, ip=10.1.5.77)   â”‚ Active/total resources: Active & requested resources: 0/2 CPU, 0.0B/4.1GiB â€¦ [repeated 3x across cluster]
2026-01-26 16:01:44.883 | (RayTrainWorker pid=501, ip=10.1.5.77) Exiting prefetcher's background thread [repeated 3x across cluster]
2026-01-26 16:01:45.886 | (RayTrainWorker pid=501, ip=10.1.5.77)  âœ”ï¸  Dataset dataset_9_0 execution finished in 0.00 seconds 10â€¦ 1.50k/â€¦   ? rowâ€¦ 
2026-01-26 16:01:45.886 | (RayTrainWorker pid=501, ip=10.1.5.77) Reporting training result 2: TrainingReport(checkpoint=None, metrics={'validation-mlogloss': 0.05672776946425438, 'validation-merror': 0.004}, validation_spec=None) [repeated 3x across cluster]
2026-01-26 16:01:45.886 | (RayTrainWorker pid=501, ip=10.1.5.77) [xgboost] Worker train_time_sec=2.64
2026-01-26 16:01:48.898 | 2026-01-26 14:01:48,897	SUCC cli.py:65 -- ---------------------------------
2026-01-26 16:01:48.898 | 2026-01-26 14:01:48,897	SUCC cli.py:66 -- Job 'kuberay-job-hw8k8' succeeded
2026-01-26 16:01:48.898 | 2026-01-26 14:01:48,897	SUCC cli.py:67 -- ---------------------------------