2026-01-16 15:47:34.172 | WARNING: Using incubator modules: jdk.incubator.vector
2026-01-16 15:47:35.167 | Files local:///app/repo/k3s/spark/main.py from /app/.worktrees/57cacef1e040b2082760a87392efe8e75cdcc8ca/k3s/spark/main.py to /opt/spark/work-dir/main.py
2026-01-16 15:47:35.227 | 26/01/16 21:47:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2026-01-16 15:47:35.901 | 2026-01-16 21:47:35,900 - SparkPreprocessing - DEBUG - Parameters retrieved from /app/repo/k3s/params.yaml
2026-01-16 15:47:36.345 | 2026-01-16 21:47:36,345 - SparkPreprocessing - INFO - Minio connection verified. Buckets: ['frontend-crm-bucket', 'k8s-mlops-platform-bucket']
2026-01-16 15:47:36.345 | 2026-01-16 21:47:36,345 - SparkPreprocessing - INFO - Creating SparkSession with S3A (MinIO) support
2026-01-16 15:47:36.634 | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
2026-01-16 15:47:36.637 | 26/01/16 21:47:36 INFO SparkContext: Running Spark version 4.0.1
2026-01-16 15:47:36.639 | 26/01/16 21:47:36 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64
2026-01-16 15:47:36.639 | 26/01/16 21:47:36 INFO SparkContext: Java version 17.0.17
2026-01-16 15:47:36.661 | 26/01/16 21:47:36 INFO ResourceUtils: ==============================================================
2026-01-16 15:47:36.662 | 26/01/16 21:47:36 INFO ResourceUtils: No custom resources configured for spark.driver.
2026-01-16 15:47:36.662 | 26/01/16 21:47:36 INFO ResourceUtils: ==============================================================
2026-01-16 15:47:36.663 | 26/01/16 21:47:36 INFO SparkContext: Submitted application: spark-preprocessing
2026-01-16 15:47:36.684 | 26/01/16 21:47:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2026-01-16 15:47:36.688 | 26/01/16 21:47:36 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
2026-01-16 15:47:36.691 | 26/01/16 21:47:36 INFO ResourceProfileManager: Added ResourceProfile id: 0
2026-01-16 15:47:36.727 | 26/01/16 21:47:36 INFO SecurityManager: Changing view acls to: spark
2026-01-16 15:47:36.728 | 26/01/16 21:47:36 INFO SecurityManager: Changing modify acls to: spark
2026-01-16 15:47:36.728 | 26/01/16 21:47:36 INFO SecurityManager: Changing view acls groups to: spark
2026-01-16 15:47:36.729 | 26/01/16 21:47:36 INFO SecurityManager: Changing modify acls groups to: spark
2026-01-16 15:47:36.731 | 26/01/16 21:47:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY; RPC SSL disabled
2026-01-16 15:47:36.932 | 26/01/16 21:47:36 INFO Utils: Successfully started service 'sparkDriver' on port 7078.
2026-01-16 15:47:36.951 | 26/01/16 21:47:36 INFO SparkEnv: Registering MapOutputTracker
2026-01-16 15:47:36.961 | 26/01/16 21:47:36 INFO SparkEnv: Registering BlockManagerMaster
2026-01-16 15:47:36.972 | 26/01/16 21:47:36 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2026-01-16 15:47:36.973 | 26/01/16 21:47:36 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2026-01-16 15:47:36.976 | 26/01/16 21:47:36 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
2026-01-16 15:47:36.990 | 26/01/16 21:47:36 INFO DiskBlockManager: Created local directory at /var/data/spark-d0daf40b-57c7-4252-a1aa-4cadf010bebd/blockmgr-608aa7fa-d694-4888-abde-deb74078941c
2026-01-16 15:47:37.007 | 26/01/16 21:47:37 INFO SparkEnv: Registering OutputCommitCoordinator
2026-01-16 15:47:37.101 | 26/01/16 21:47:37 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
2026-01-16 15:47:37.169 | 26/01/16 21:47:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
2026-01-16 15:47:37.196 | 26/01/16 21:47:37 WARN SparkContext: File with 'local' scheme local:/app/repo/k3s/spark/main.py is not supported to add to file server, since it is already available on every node.
2026-01-16 15:47:37.213 | 26/01/16 21:47:37 INFO SecurityManager: Changing view acls to: spark
2026-01-16 15:47:37.214 | 26/01/16 21:47:37 INFO SecurityManager: Changing modify acls to: spark
2026-01-16 15:47:37.214 | 26/01/16 21:47:37 INFO SecurityManager: Changing view acls groups to: spark
2026-01-16 15:47:37.214 | 26/01/16 21:47:37 INFO SecurityManager: Changing modify acls groups to: spark
2026-01-16 15:47:37.214 | 26/01/16 21:47:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY; RPC SSL disabled
2026-01-16 15:47:37.254 | 26/01/16 21:47:37 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
2026-01-16 15:47:38.226 | 26/01/16 21:47:38 INFO ExecutorPodsAllocator: Going to request 2 executors from Kubernetes for ResourceProfile Id: 0, target: 2, known: 0, sharedSlotFromPendingPods: 2147483647.
2026-01-16 15:47:38.272 | 26/01/16 21:47:38 INFO KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : spark.kubernetes.namespace
2026-01-16 15:47:38.431 | 26/01/16 21:47:38 INFO ExecutorPodsAllocator: Found 0 reusable PVCs from 0 PVCs
2026-01-16 15:47:38.474 | 26/01/16 21:47:38 INFO KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : spark.kubernetes.namespace
2026-01-16 15:47:38.475 | 26/01/16 21:47:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.
2026-01-16 15:47:38.478 | 26/01/16 21:47:38 INFO NettyBlockTransferService: Server created on spark-app-0-driver-svc.spark.svc 10.1.4.94:7079
2026-01-16 15:47:38.481 | 26/01/16 21:47:38 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2026-01-16 15:47:38.483 | 26/01/16 21:47:38 INFO BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script
2026-01-16 15:47:38.494 | 26/01/16 21:47:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-app-0-driver-svc.spark.svc, 7079, None)
2026-01-16 15:47:38.499 | 26/01/16 21:47:38 INFO BlockManagerMasterEndpoint: Registering block manager spark-app-0-driver-svc.spark.svc:7079 with 413.9 MiB RAM, BlockManagerId(driver, spark-app-0-driver-svc.spark.svc, 7079, None)
2026-01-16 15:47:38.501 | 26/01/16 21:47:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-app-0-driver-svc.spark.svc, 7079, None)
2026-01-16 15:47:38.502 | 26/01/16 21:47:38 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-app-0-driver-svc.spark.svc, 7079, None)
2026-01-16 15:47:38.556 | 26/01/16 21:47:38 INFO KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : spark.kubernetes.namespace
2026-01-16 15:47:38.558 | 26/01/16 21:47:38 INFO BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script
2026-01-16 15:47:43.393 | 26/01/16 21:47:43 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.1.4.95:46720) with ID 1, ResourceProfileId 0
2026-01-16 15:47:43.400 | 26/01/16 21:47:43 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.1.4.96:53306) with ID 2, ResourceProfileId 0
2026-01-16 15:47:43.498 | 26/01/16 21:47:43 INFO KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
2026-01-16 15:47:43.522 | 26/01/16 21:47:43 INFO BlockManagerMasterEndpoint: Registering block manager 10.1.4.95:45803 with 413.9 MiB RAM, BlockManagerId(1, 10.1.4.95, 45803, None)
2026-01-16 15:47:43.529 | 26/01/16 21:47:43 INFO BlockManagerMasterEndpoint: Registering block manager 10.1.4.96:39091 with 413.9 MiB RAM, BlockManagerId(2, 10.1.4.96, 39091, None)
2026-01-16 15:47:45.043 | 2026-01-16 21:47:45,042 - SparkPreprocessing - INFO - Loading feature pipeline: preprocessing.preprocessing_001
2026-01-16 15:47:45.207 | 2026-01-16 21:47:45,207 - SparkPreprocessing - INFO - Loading data from s3a://k8s-mlops-platform-bucket/v1/raw/train/
2026-01-16 15:47:45.259 | 26/01/16 21:47:45 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2026-01-16 15:47:45.264 | 26/01/16 21:47:45 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
2026-01-16 15:47:45.816 | 26/01/16 21:47:45 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
2026-01-16 15:47:45.823 | 26/01/16 21:47:45 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2026-01-16 15:47:45.823 | 26/01/16 21:47:45 INFO MetricsSystemImpl: s3a-file-system metrics system started
2026-01-16 15:47:47.390 | 26/01/16 21:47:47 INFO HadoopFSUtils: Listing s3a://k8s-mlops-platform-bucket/v1/raw/train with listFiles API
2026-01-16 15:47:47.534 | 26/01/16 21:47:47 INFO InMemoryFileIndex: It took 147 ms to list leaf files for 1 paths.
2026-01-16 15:47:47.919 | 26/01/16 21:47:47 INFO FileSourceStrategy: Pushed Filters: 
2026-01-16 15:47:47.922 | 26/01/16 21:47:47 INFO FileSourceStrategy: Post-Scan Filters: Set()
2026-01-16 15:47:48.214 | 26/01/16 21:47:48 INFO CodeGenerator: Code generated in 160.677202 ms
2026-01-16 15:47:48.237 | 26/01/16 21:47:48 INFO MemoryStore: MemoryStore started with capacity 413.9 MiB
2026-01-16 15:47:48.261 | 26/01/16 21:47:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 237.9 KiB, free 413.7 MiB)
2026-01-16 15:47:48.298 | 26/01/16 21:47:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 41.4 KiB, free 413.7 MiB)
2026-01-16 15:47:48.303 | 26/01/16 21:47:48 INFO SparkContext: Created broadcast 0 from javaToPython at NativeMethodAccessorImpl.java:0
2026-01-16 15:47:48.318 | 26/01/16 21:47:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2026-01-16 15:47:48.421 | 2026-01-16 21:47:48,421 - SparkPreprocessing - INFO - Data loaded successfully | partitions: 1
2026-01-16 15:47:48.938 | 26/01/16 21:47:48 INFO FileSourceStrategy: Pushed Filters: 
2026-01-16 15:47:48.938 | 26/01/16 21:47:48 INFO FileSourceStrategy: Post-Scan Filters: Set()
2026-01-16 15:47:49.257 | 26/01/16 21:47:49 INFO CodeGenerator: Code generated in 83.965179 ms
2026-01-16 15:47:49.263 | 26/01/16 21:47:49 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 237.0 KiB, free 413.4 MiB)
2026-01-16 15:47:49.274 | 26/01/16 21:47:49 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 41.2 KiB, free 413.4 MiB)
2026-01-16 15:47:49.275 | 26/01/16 21:47:49 INFO SparkContext: Created broadcast 1 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
2026-01-16 15:47:49.276 | 26/01/16 21:47:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2026-01-16 15:47:49.389 | 26/01/16 21:47:49 INFO DAGScheduler: Registering RDD 9 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) as input to shuffle 0
2026-01-16 15:47:49.393 | 26/01/16 21:47:49 INFO DAGScheduler: Got map stage job 0 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions
2026-01-16 15:47:49.394 | 26/01/16 21:47:49 INFO DAGScheduler: Final stage: ShuffleMapStage 0 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
2026-01-16 15:47:49.394 | 26/01/16 21:47:49 INFO DAGScheduler: Parents of final stage: List()
2026-01-16 15:47:49.395 | 26/01/16 21:47:49 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:47:49.399 | 26/01/16 21:47:49 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[9] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
2026-01-16 15:47:49.421 | 26/01/16 21:47:49 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 39.0 KiB, free 413.3 MiB)
2026-01-16 15:47:49.423 | 26/01/16 21:47:49 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 413.3 MiB)
2026-01-16 15:47:49.425 | 26/01/16 21:47:49 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:47:49.442 | 26/01/16 21:47:49 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[9] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:47:49.451 | 26/01/16 21:47:49 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
2026-01-16 15:47:49.476 | 26/01/16 21:47:49 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.1.4.96,executor 2, partition 0, PROCESS_LOCAL, 10233 bytes) 
2026-01-16 15:47:52.670 | 26/01/16 21:47:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3205 ms on 10.1.4.96 (executor 2) (1/1)
2026-01-16 15:47:52.671 | 26/01/16 21:47:52 INFO TaskSchedulerImpl: Removed TaskSet 0.0 whose tasks have all completed, from pool 
2026-01-16 15:47:52.676 | 26/01/16 21:47:52 INFO DAGScheduler: ShuffleMapStage 0 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 3270 ms
2026-01-16 15:47:52.677 | 26/01/16 21:47:52 INFO DAGScheduler: looking for newly runnable stages
2026-01-16 15:47:52.677 | 26/01/16 21:47:52 INFO DAGScheduler: running: HashSet()
2026-01-16 15:47:52.678 | 26/01/16 21:47:52 INFO DAGScheduler: waiting: HashSet()
2026-01-16 15:47:52.678 | 26/01/16 21:47:52 INFO DAGScheduler: failed: HashSet()
2026-01-16 15:47:52.699 | 26/01/16 21:47:52 INFO ShufflePartitionsUtil: For shuffle(0, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2026-01-16 15:47:52.719 | 26/01/16 21:47:52 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2026-01-16 15:47:52.750 | 26/01/16 21:47:52 INFO CodeGenerator: Code generated in 19.690523 ms
2026-01-16 15:47:52.769 | 26/01/16 21:47:52 INFO DAGScheduler: Registering RDD 13 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) as input to shuffle 1
2026-01-16 15:47:52.769 | 26/01/16 21:47:52 INFO DAGScheduler: Got map stage job 1 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions
2026-01-16 15:47:52.769 | 26/01/16 21:47:52 INFO DAGScheduler: Final stage: ShuffleMapStage 2 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
2026-01-16 15:47:52.770 | 26/01/16 21:47:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
2026-01-16 15:47:52.770 | 26/01/16 21:47:52 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:47:52.771 | 26/01/16 21:47:52 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
2026-01-16 15:47:52.784 | 26/01/16 21:47:52 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 43.6 KiB, free 413.3 MiB)
2026-01-16 15:47:52.797 | 26/01/16 21:47:52 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 18.9 KiB, free 413.3 MiB)
2026-01-16 15:47:52.799 | 26/01/16 21:47:52 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:47:52.800 | 26/01/16 21:47:52 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:47:52.800 | 26/01/16 21:47:52 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
2026-01-16 15:47:52.805 | 26/01/16 21:47:52 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (10.1.4.96,executor 2, partition 0, NODE_LOCAL, 9612 bytes) 
2026-01-16 15:47:52.893 | 26/01/16 21:47:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.1.4.96:53306
2026-01-16 15:47:53.090 | 26/01/16 21:47:53 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 286 ms on 10.1.4.96 (executor 2) (1/1)
2026-01-16 15:47:53.090 | 26/01/16 21:47:53 INFO TaskSchedulerImpl: Removed TaskSet 2.0 whose tasks have all completed, from pool 
2026-01-16 15:47:53.091 | 26/01/16 21:47:53 INFO DAGScheduler: ShuffleMapStage 2 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 313 ms
2026-01-16 15:47:53.091 | 26/01/16 21:47:53 INFO DAGScheduler: looking for newly runnable stages
2026-01-16 15:47:53.091 | 26/01/16 21:47:53 INFO DAGScheduler: running: HashSet()
2026-01-16 15:47:53.091 | 26/01/16 21:47:53 INFO DAGScheduler: waiting: HashSet()
2026-01-16 15:47:53.091 | 26/01/16 21:47:53 INFO DAGScheduler: failed: HashSet()
2026-01-16 15:47:53.095 | 26/01/16 21:47:53 INFO ShufflePartitionsUtil: For shuffle(1, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2026-01-16 15:47:53.206 | 26/01/16 21:47:53 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
2026-01-16 15:47:53.209 | 26/01/16 21:47:53 INFO DAGScheduler: Got job 2 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions
2026-01-16 15:47:53.210 | 26/01/16 21:47:53 INFO DAGScheduler: Final stage: ResultStage 5 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
2026-01-16 15:47:53.210 | 26/01/16 21:47:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
2026-01-16 15:47:53.211 | 26/01/16 21:47:53 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:47:53.212 | 26/01/16 21:47:53 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[16] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
2026-01-16 15:47:53.218 | 26/01/16 21:47:53 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 42.3 KiB, free 413.3 MiB)
2026-01-16 15:47:53.219 | 26/01/16 21:47:53 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 17.7 KiB, free 413.3 MiB)
2026-01-16 15:47:53.221 | 26/01/16 21:47:53 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:47:53.223 | 26/01/16 21:47:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[16] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:47:53.223 | 26/01/16 21:47:53 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
2026-01-16 15:47:53.225 | 26/01/16 21:47:53 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 2) (10.1.4.96,executor 2, partition 0, NODE_LOCAL, 9623 bytes) 
2026-01-16 15:47:53.300 | 26/01/16 21:47:53 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.1.4.96:53306
2026-01-16 15:47:53.371 | 26/01/16 21:47:53 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 2) in 146 ms on 10.1.4.96 (executor 2) (1/1)
2026-01-16 15:47:53.371 | 26/01/16 21:47:53 INFO TaskSchedulerImpl: Removed TaskSet 5.0 whose tasks have all completed, from pool 
2026-01-16 15:47:53.372 | 26/01/16 21:47:53 INFO DAGScheduler: ResultStage 5 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 157 ms
2026-01-16 15:47:53.374 | 26/01/16 21:47:53 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
2026-01-16 15:47:53.374 | 26/01/16 21:47:53 INFO TaskSchedulerImpl: Canceling stage 5
2026-01-16 15:47:53.375 | 26/01/16 21:47:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
2026-01-16 15:47:53.377 | 26/01/16 21:47:53 INFO DAGScheduler: Job 2 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 170.717004 ms
2026-01-16 15:47:53.418 | 26/01/16 21:47:53 INFO CodeGenerator: Code generated in 13.759638 ms
2026-01-16 15:47:53.529 | 26/01/16 21:47:53 INFO FileSourceStrategy: Pushed Filters: 
2026-01-16 15:47:53.530 | 26/01/16 21:47:53 INFO FileSourceStrategy: Post-Scan Filters: Set()
2026-01-16 15:47:53.563 | 26/01/16 21:47:53 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 237.0 KiB, free 413.1 MiB)
2026-01-16 15:47:53.572 | 26/01/16 21:47:53 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 41.3 KiB, free 413.1 MiB)
2026-01-16 15:47:53.573 | 26/01/16 21:47:53 INFO SparkContext: Created broadcast 5 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
2026-01-16 15:47:53.574 | 26/01/16 21:47:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2026-01-16 15:47:53.580 | 26/01/16 21:47:53 INFO DAGScheduler: Registering RDD 20 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) as input to shuffle 2
2026-01-16 15:47:53.580 | 26/01/16 21:47:53 INFO DAGScheduler: Got map stage job 3 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions
2026-01-16 15:47:53.580 | 26/01/16 21:47:53 INFO DAGScheduler: Final stage: ShuffleMapStage 6 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
2026-01-16 15:47:53.580 | 26/01/16 21:47:53 INFO DAGScheduler: Parents of final stage: List()
2026-01-16 15:47:53.581 | 26/01/16 21:47:53 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:47:53.581 | 26/01/16 21:47:53 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[20] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
2026-01-16 15:47:53.584 | 26/01/16 21:47:53 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 39.0 KiB, free 413.0 MiB)
2026-01-16 15:47:53.586 | 26/01/16 21:47:53 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 413.0 MiB)
2026-01-16 15:47:53.587 | 26/01/16 21:47:53 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:47:53.588 | 26/01/16 21:47:53 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[20] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:47:53.588 | 26/01/16 21:47:53 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
2026-01-16 15:47:53.590 | 26/01/16 21:47:53 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 3) (10.1.4.95,executor 1, partition 0, PROCESS_LOCAL, 10233 bytes) 
2026-01-16 15:47:56.693 | 26/01/16 21:47:56 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 3) in 3104 ms on 10.1.4.95 (executor 1) (1/1)
2026-01-16 15:47:56.693 | 26/01/16 21:47:56 INFO TaskSchedulerImpl: Removed TaskSet 6.0 whose tasks have all completed, from pool 
2026-01-16 15:47:56.694 | 26/01/16 21:47:56 INFO DAGScheduler: ShuffleMapStage 6 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 3112 ms
2026-01-16 15:47:56.694 | 26/01/16 21:47:56 INFO DAGScheduler: looking for newly runnable stages
2026-01-16 15:47:56.694 | 26/01/16 21:47:56 INFO DAGScheduler: running: HashSet()
2026-01-16 15:47:56.694 | 26/01/16 21:47:56 INFO DAGScheduler: waiting: HashSet()
2026-01-16 15:47:56.694 | 26/01/16 21:47:56 INFO DAGScheduler: failed: HashSet()
2026-01-16 15:47:56.699 | 26/01/16 21:47:56 INFO ShufflePartitionsUtil: For shuffle(2, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2026-01-16 15:47:56.703 | 26/01/16 21:47:56 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2026-01-16 15:47:56.709 | 26/01/16 21:47:56 INFO DAGScheduler: Registering RDD 24 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) as input to shuffle 3
2026-01-16 15:47:56.709 | 26/01/16 21:47:56 INFO DAGScheduler: Got map stage job 4 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions
2026-01-16 15:47:56.709 | 26/01/16 21:47:56 INFO DAGScheduler: Final stage: ShuffleMapStage 8 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
2026-01-16 15:47:56.709 | 26/01/16 21:47:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
2026-01-16 15:47:56.709 | 26/01/16 21:47:56 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:47:56.710 | 26/01/16 21:47:56 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[24] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
2026-01-16 15:47:56.713 | 26/01/16 21:47:56 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 43.6 KiB, free 413.0 MiB)
2026-01-16 15:47:56.715 | 26/01/16 21:47:56 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 18.9 KiB, free 412.9 MiB)
2026-01-16 15:47:56.716 | 26/01/16 21:47:56 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:47:56.716 | 26/01/16 21:47:56 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[24] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:47:56.717 | 26/01/16 21:47:56 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
2026-01-16 15:47:56.719 | 26/01/16 21:47:56 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 4) (10.1.4.95,executor 1, partition 0, NODE_LOCAL, 9612 bytes) 
2026-01-16 15:47:56.782 | 26/01/16 21:47:56 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.1.4.95:46720
2026-01-16 15:47:56.937 | 26/01/16 21:47:56 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 4) in 218 ms on 10.1.4.95 (executor 1) (1/1)
2026-01-16 15:47:56.938 | 26/01/16 21:47:56 INFO TaskSchedulerImpl: Removed TaskSet 8.0 whose tasks have all completed, from pool 
2026-01-16 15:47:56.939 | 26/01/16 21:47:56 INFO DAGScheduler: ShuffleMapStage 8 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 229 ms
2026-01-16 15:47:56.939 | 26/01/16 21:47:56 INFO DAGScheduler: looking for newly runnable stages
2026-01-16 15:47:56.939 | 26/01/16 21:47:56 INFO DAGScheduler: running: HashSet()
2026-01-16 15:47:56.939 | 26/01/16 21:47:56 INFO DAGScheduler: waiting: HashSet()
2026-01-16 15:47:56.939 | 26/01/16 21:47:56 INFO DAGScheduler: failed: HashSet()
2026-01-16 15:47:56.942 | 26/01/16 21:47:56 INFO ShufflePartitionsUtil: For shuffle(3, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2026-01-16 15:47:57.110 | 26/01/16 21:47:57 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
2026-01-16 15:47:57.111 | 26/01/16 21:47:57 INFO DAGScheduler: Got job 5 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions
2026-01-16 15:47:57.111 | 26/01/16 21:47:57 INFO DAGScheduler: Final stage: ResultStage 11 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
2026-01-16 15:47:57.111 | 26/01/16 21:47:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
2026-01-16 15:47:57.112 | 26/01/16 21:47:57 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:47:57.113 | 26/01/16 21:47:57 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[27] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
2026-01-16 15:47:57.116 | 26/01/16 21:47:57 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 42.3 KiB, free 412.9 MiB)
2026-01-16 15:47:57.118 | 26/01/16 21:47:57 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 17.7 KiB, free 412.9 MiB)
2026-01-16 15:47:57.119 | 26/01/16 21:47:57 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:47:57.120 | 26/01/16 21:47:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[27] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:47:57.120 | 26/01/16 21:47:57 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
2026-01-16 15:47:57.121 | 26/01/16 21:47:57 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 5) (10.1.4.95,executor 1, partition 0, NODE_LOCAL, 9623 bytes) 
2026-01-16 15:47:57.194 | 26/01/16 21:47:57 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.1.4.95:46720
2026-01-16 15:47:57.259 | 26/01/16 21:47:57 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 5) in 138 ms on 10.1.4.95 (executor 1) (1/1)
2026-01-16 15:47:57.259 | 26/01/16 21:47:57 INFO TaskSchedulerImpl: Removed TaskSet 11.0 whose tasks have all completed, from pool 
2026-01-16 15:47:57.260 | 26/01/16 21:47:57 INFO DAGScheduler: ResultStage 11 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 146 ms
2026-01-16 15:47:57.260 | 26/01/16 21:47:57 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
2026-01-16 15:47:57.260 | 26/01/16 21:47:57 INFO TaskSchedulerImpl: Canceling stage 11
2026-01-16 15:47:57.260 | 26/01/16 21:47:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
2026-01-16 15:47:57.261 | 26/01/16 21:47:57 INFO DAGScheduler: Job 5 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 150.421868 ms
2026-01-16 15:47:57.334 | 26/01/16 21:47:57 INFO FileSourceStrategy: Pushed Filters: 
2026-01-16 15:47:57.334 | 26/01/16 21:47:57 INFO FileSourceStrategy: Post-Scan Filters: Set()
2026-01-16 15:47:57.384 | 26/01/16 21:47:57 INFO CodeGenerator: Code generated in 22.482904 ms
2026-01-16 15:47:57.387 | 26/01/16 21:47:57 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 237.1 KiB, free 412.8 MiB)
2026-01-16 15:47:57.395 | 26/01/16 21:47:57 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 41.2 KiB, free 412.8 MiB)
2026-01-16 15:47:57.396 | 26/01/16 21:47:57 INFO SparkContext: Created broadcast 9 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
2026-01-16 15:47:57.397 | 26/01/16 21:47:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2026-01-16 15:47:57.402 | 26/01/16 21:47:57 INFO DAGScheduler: Registering RDD 31 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) as input to shuffle 4
2026-01-16 15:47:57.402 | 26/01/16 21:47:57 INFO DAGScheduler: Got map stage job 6 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions
2026-01-16 15:47:57.402 | 26/01/16 21:47:57 INFO DAGScheduler: Final stage: ShuffleMapStage 12 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
2026-01-16 15:47:57.402 | 26/01/16 21:47:57 INFO DAGScheduler: Parents of final stage: List()
2026-01-16 15:47:57.403 | 26/01/16 21:47:57 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:47:57.403 | 26/01/16 21:47:57 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[31] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
2026-01-16 15:47:57.406 | 26/01/16 21:47:57 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 40.4 KiB, free 412.7 MiB)
2026-01-16 15:47:57.408 | 26/01/16 21:47:57 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 412.7 MiB)
2026-01-16 15:47:57.409 | 26/01/16 21:47:57 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:47:57.410 | 26/01/16 21:47:57 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[31] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:47:57.410 | 26/01/16 21:47:57 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
2026-01-16 15:47:57.411 | 26/01/16 21:47:57 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 6) (10.1.4.96,executor 2, partition 0, PROCESS_LOCAL, 10233 bytes) 
2026-01-16 15:47:58.469 | 26/01/16 21:47:58 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 6) in 1058 ms on 10.1.4.96 (executor 2) (1/1)
2026-01-16 15:47:58.469 | 26/01/16 21:47:58 INFO TaskSchedulerImpl: Removed TaskSet 12.0 whose tasks have all completed, from pool 
2026-01-16 15:47:58.470 | 26/01/16 21:47:58 INFO DAGScheduler: ShuffleMapStage 12 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 1065 ms
2026-01-16 15:47:58.470 | 26/01/16 21:47:58 INFO DAGScheduler: looking for newly runnable stages
2026-01-16 15:47:58.470 | 26/01/16 21:47:58 INFO DAGScheduler: running: HashSet()
2026-01-16 15:47:58.470 | 26/01/16 21:47:58 INFO DAGScheduler: waiting: HashSet()
2026-01-16 15:47:58.470 | 26/01/16 21:47:58 INFO DAGScheduler: failed: HashSet()
2026-01-16 15:47:58.474 | 26/01/16 21:47:58 INFO ShufflePartitionsUtil: For shuffle(4, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2026-01-16 15:47:58.479 | 26/01/16 21:47:58 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2026-01-16 15:47:58.484 | 26/01/16 21:47:58 INFO DAGScheduler: Registering RDD 35 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) as input to shuffle 5
2026-01-16 15:47:58.484 | 26/01/16 21:47:58 INFO DAGScheduler: Got map stage job 7 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions
2026-01-16 15:47:58.484 | 26/01/16 21:47:58 INFO DAGScheduler: Final stage: ShuffleMapStage 14 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
2026-01-16 15:47:58.484 | 26/01/16 21:47:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
2026-01-16 15:47:58.484 | 26/01/16 21:47:58 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:47:58.485 | 26/01/16 21:47:58 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[35] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
2026-01-16 15:47:58.486 | 26/01/16 21:47:58 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 44.6 KiB, free 412.7 MiB)
2026-01-16 15:47:58.488 | 26/01/16 21:47:58 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 19.3 KiB, free 412.7 MiB)
2026-01-16 15:47:58.489 | 26/01/16 21:47:58 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:47:58.489 | 26/01/16 21:47:58 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[35] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:47:58.489 | 26/01/16 21:47:58 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
2026-01-16 15:47:58.490 | 26/01/16 21:47:58 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 7) (10.1.4.96,executor 2, partition 0, NODE_LOCAL, 9612 bytes) 
2026-01-16 15:47:58.510 | 26/01/16 21:47:58 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.1.4.96:53306
2026-01-16 15:47:58.538 | 26/01/16 21:47:58 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 7) in 48 ms on 10.1.4.96 (executor 2) (1/1)
2026-01-16 15:47:58.539 | 26/01/16 21:47:58 INFO TaskSchedulerImpl: Removed TaskSet 14.0 whose tasks have all completed, from pool 
2026-01-16 15:47:58.540 | 26/01/16 21:47:58 INFO DAGScheduler: ShuffleMapStage 14 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 54 ms
2026-01-16 15:47:58.540 | 26/01/16 21:47:58 INFO DAGScheduler: looking for newly runnable stages
2026-01-16 15:47:58.540 | 26/01/16 21:47:58 INFO DAGScheduler: running: HashSet()
2026-01-16 15:47:58.540 | 26/01/16 21:47:58 INFO DAGScheduler: waiting: HashSet()
2026-01-16 15:47:58.540 | 26/01/16 21:47:58 INFO DAGScheduler: failed: HashSet()
2026-01-16 15:47:58.542 | 26/01/16 21:47:58 INFO ShufflePartitionsUtil: For shuffle(5, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2026-01-16 15:47:58.560 | 26/01/16 21:47:58 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
2026-01-16 15:47:58.562 | 26/01/16 21:47:58 INFO DAGScheduler: Got job 8 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions
2026-01-16 15:47:58.562 | 26/01/16 21:47:58 INFO DAGScheduler: Final stage: ResultStage 17 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
2026-01-16 15:47:58.562 | 26/01/16 21:47:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
2026-01-16 15:47:58.562 | 26/01/16 21:47:58 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:47:58.563 | 26/01/16 21:47:58 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[38] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
2026-01-16 15:47:58.566 | 26/01/16 21:47:58 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 43.2 KiB, free 412.6 MiB)
2026-01-16 15:47:58.576 | 26/01/16 21:47:58 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 18.2 KiB, free 412.6 MiB)
2026-01-16 15:47:58.578 | 26/01/16 21:47:58 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:47:58.579 | 26/01/16 21:47:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[38] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:47:58.579 | 26/01/16 21:47:58 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
2026-01-16 15:47:58.581 | 26/01/16 21:47:58 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 8) (10.1.4.96,executor 2, partition 0, NODE_LOCAL, 9623 bytes) 
2026-01-16 15:47:58.607 | 26/01/16 21:47:58 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.1.4.96:53306
2026-01-16 15:47:58.635 | 26/01/16 21:47:58 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 8) in 54 ms on 10.1.4.96 (executor 2) (1/1)
2026-01-16 15:47:58.635 | 26/01/16 21:47:58 INFO TaskSchedulerImpl: Removed TaskSet 17.0 whose tasks have all completed, from pool 
2026-01-16 15:47:58.636 | 26/01/16 21:47:58 INFO DAGScheduler: ResultStage 17 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 73 ms
2026-01-16 15:47:58.637 | 26/01/16 21:47:58 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
2026-01-16 15:47:58.637 | 26/01/16 21:47:58 INFO TaskSchedulerImpl: Canceling stage 17
2026-01-16 15:47:58.637 | 26/01/16 21:47:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
2026-01-16 15:47:58.638 | 26/01/16 21:47:58 INFO DAGScheduler: Job 8 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 77.906483 ms
2026-01-16 15:47:58.786 | 26/01/16 21:47:58 INFO FileSourceStrategy: Pushed Filters: 
2026-01-16 15:47:58.786 | 26/01/16 21:47:58 INFO FileSourceStrategy: Post-Scan Filters: Set()
2026-01-16 15:47:58.797 | 26/01/16 21:47:58 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2026-01-16 15:47:58.900 | 26/01/16 21:47:58 INFO CodeGenerator: Code generated in 32.97901 ms
2026-01-16 15:47:58.904 | 26/01/16 21:47:58 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 237.8 KiB, free 412.8 MiB)
2026-01-16 15:47:58.912 | 26/01/16 21:47:58 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 41.3 KiB, free 412.8 MiB)
2026-01-16 15:47:58.913 | 26/01/16 21:47:58 INFO SparkContext: Created broadcast 13 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
2026-01-16 15:47:58.913 | 26/01/16 21:47:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2026-01-16 15:47:58.960 | 26/01/16 21:47:58 INFO DAGScheduler: Registering RDD 43 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) as input to shuffle 6
2026-01-16 15:47:58.960 | 26/01/16 21:47:58 INFO DAGScheduler: Got map stage job 9 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions
2026-01-16 15:47:58.960 | 26/01/16 21:47:58 INFO DAGScheduler: Final stage: ShuffleMapStage 18 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
2026-01-16 15:47:58.960 | 26/01/16 21:47:58 INFO DAGScheduler: Parents of final stage: List()
2026-01-16 15:47:58.960 | 26/01/16 21:47:58 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:47:58.961 | 26/01/16 21:47:58 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[43] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
2026-01-16 15:47:58.986 | 26/01/16 21:47:58 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 83.9 KiB, free 413.0 MiB)
2026-01-16 15:47:58.988 | 26/01/16 21:47:58 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 31.0 KiB, free 412.9 MiB)
2026-01-16 15:47:58.989 | 26/01/16 21:47:58 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:47:58.990 | 26/01/16 21:47:58 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[43] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:47:58.990 | 26/01/16 21:47:58 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
2026-01-16 15:47:58.992 | 26/01/16 21:47:58 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 9) (10.1.4.96,executor 2, partition 0, PROCESS_LOCAL, 10233 bytes) 
2026-01-16 15:48:00.881 | 26/01/16 21:48:00 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 9) in 1889 ms on 10.1.4.96 (executor 2) (1/1)
2026-01-16 15:48:00.881 | 26/01/16 21:48:00 INFO TaskSchedulerImpl: Removed TaskSet 18.0 whose tasks have all completed, from pool 
2026-01-16 15:48:00.881 | 26/01/16 21:48:00 INFO DAGScheduler: ShuffleMapStage 18 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 1920 ms
2026-01-16 15:48:00.881 | 26/01/16 21:48:00 INFO DAGScheduler: looking for newly runnable stages
2026-01-16 15:48:00.881 | 26/01/16 21:48:00 INFO DAGScheduler: running: HashSet()
2026-01-16 15:48:00.882 | 26/01/16 21:48:00 INFO DAGScheduler: waiting: HashSet()
2026-01-16 15:48:00.882 | 26/01/16 21:48:00 INFO DAGScheduler: failed: HashSet()
2026-01-16 15:48:00.900 | 26/01/16 21:48:00 INFO CodeGenerator: Code generated in 8.491014 ms
2026-01-16 15:48:00.912 | 26/01/16 21:48:00 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
2026-01-16 15:48:00.913 | 26/01/16 21:48:00 INFO DAGScheduler: Got job 10 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions
2026-01-16 15:48:00.913 | 26/01/16 21:48:00 INFO DAGScheduler: Final stage: ResultStage 20 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
2026-01-16 15:48:00.913 | 26/01/16 21:48:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)
2026-01-16 15:48:00.914 | 26/01/16 21:48:00 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:48:00.914 | 26/01/16 21:48:00 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[47] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
2026-01-16 15:48:00.918 | 26/01/16 21:48:00 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 77.2 KiB, free 412.9 MiB)
2026-01-16 15:48:00.920 | 26/01/16 21:48:00 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 29.4 KiB, free 412.9 MiB)
2026-01-16 15:48:00.920 | 26/01/16 21:48:00 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:48:00.921 | 26/01/16 21:48:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[47] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:48:00.921 | 26/01/16 21:48:00 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
2026-01-16 15:48:00.922 | 26/01/16 21:48:00 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 10) (10.1.4.96,executor 2, partition 0, NODE_LOCAL, 9623 bytes) 
2026-01-16 15:48:00.943 | 26/01/16 21:48:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.1.4.96:53306
2026-01-16 15:48:00.987 | 26/01/16 21:48:00 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 10) in 64 ms on 10.1.4.96 (executor 2) (1/1)
2026-01-16 15:48:00.987 | 26/01/16 21:48:00 INFO TaskSchedulerImpl: Removed TaskSet 20.0 whose tasks have all completed, from pool 
2026-01-16 15:48:00.988 | 26/01/16 21:48:00 INFO DAGScheduler: ResultStage 20 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 73 ms
2026-01-16 15:48:00.988 | 26/01/16 21:48:00 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
2026-01-16 15:48:00.988 | 26/01/16 21:48:00 INFO TaskSchedulerImpl: Canceling stage 20
2026-01-16 15:48:00.988 | 26/01/16 21:48:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
2026-01-16 15:48:00.988 | 26/01/16 21:48:00 INFO DAGScheduler: Job 10 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 76.20969 ms
2026-01-16 15:48:01.004 | 26/01/16 21:48:01 INFO CodeGenerator: Code generated in 5.282382 ms
2026-01-16 15:48:01.401 | 26/01/16 21:48:01 INFO FileSystemOverwrite: Path s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model already exists. It will be overwritten.
2026-01-16 15:48:03.087 | 26/01/16 21:48:03 INFO PathOutputCommitterFactory: Using schema-specific factory for s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/metadata
2026-01-16 15:48:03.088 | 26/01/16 21:48:03 INFO PathOutputCommitterFactory: Using OutputCommitter factory class class org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory from key mapreduce.outputcommitter.factory.scheme.s3a
2026-01-16 15:48:03.092 | 26/01/16 21:48:03 INFO AbstractS3ACommitter: Job UUID ddb9374a-b78b-4b09-8859-50b68db947e0 source spark.sql.sources.writeJobUUID
2026-01-16 15:48:03.097 | 26/01/16 21:48:03 INFO AbstractS3ACommitterFactory: Using committer magic to output data to s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/metadata
2026-01-16 15:48:03.098 | 26/01/16 21:48:03 INFO AbstractS3ACommitterFactory: Using Committer MagicCommitter{AbstractS3ACommitter{role=Task committer attempt_202601162148035594752130378811319_0000_m_000000_0, name=magic, outputPath=s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/metadata, workPath=s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/metadata/__magic_job-ddb9374a-b78b-4b09-8859-50b68db947e0/job-ddb9374a-b78b-4b09-8859-50b68db947e0/00/tasks/attempt_202601162148035594752130378811319_0000_m_000000_0/__base, uuid='ddb9374a-b78b-4b09-8859-50b68db947e0', uuid source=JobUUIDSource{text='spark.sql.sources.writeJobUUID'}}} for s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/metadata created by org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory@69252c0c
2026-01-16 15:48:03.098 | 26/01/16 21:48:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter
2026-01-16 15:48:03.098 | 26/01/16 21:48:03 INFO MagicS3GuardCommitter: Starting: Setup Job (no job ID)
2026-01-16 15:48:03.098 | 26/01/16 21:48:03 INFO AbstractS3ACommitter: Starting: Job ddb9374a-b78b-4b09-8859-50b68db947e0 setting up
2026-01-16 15:48:04.209 | 26/01/16 21:48:04 INFO AbstractS3ACommitter: Job ddb9374a-b78b-4b09-8859-50b68db947e0 setting up: duration 0:01.111s
2026-01-16 15:48:04.466 | 26/01/16 21:48:04 INFO MagicS3GuardCommitter: Setup Job (no job ID): duration 0:01.359s
2026-01-16 15:48:04.466 | 26/01/16 21:48:04 INFO CodeGenerator: Code generated in 2.848201 ms
2026-01-16 15:48:04.476 | 26/01/16 21:48:04 INFO SparkContext: Starting job: text at ReadWrite.scala:445
2026-01-16 15:48:04.476 | 26/01/16 21:48:04 INFO DAGScheduler: Got job 11 (text at ReadWrite.scala:445) with 1 output partitions
2026-01-16 15:48:04.476 | 26/01/16 21:48:04 INFO DAGScheduler: Final stage: ResultStage 21 (text at ReadWrite.scala:445)
2026-01-16 15:48:04.478 | 26/01/16 21:48:04 INFO DAGScheduler: Parents of final stage: List()
2026-01-16 15:48:04.478 | 26/01/16 21:48:04 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:48:04.478 | 26/01/16 21:48:04 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[50] at text at ReadWrite.scala:445), which has no missing parents
2026-01-16 15:48:04.509 | 26/01/16 21:48:04 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 238.3 KiB, free 413.1 MiB)
2026-01-16 15:48:04.511 | 26/01/16 21:48:04 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 86.8 KiB, free 413.1 MiB)
2026-01-16 15:48:04.513 | 26/01/16 21:48:04 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:48:04.513 | 26/01/16 21:48:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[50] at text at ReadWrite.scala:445) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:48:04.513 | 26/01/16 21:48:04 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
2026-01-16 15:48:04.516 | 26/01/16 21:48:04 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 11) (10.1.4.95,executor 1, partition 0, PROCESS_LOCAL, 10190 bytes) 
2026-01-16 15:48:07.289 | 26/01/16 21:48:07 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 11) in 2774 ms on 10.1.4.95 (executor 1) (1/1)
2026-01-16 15:48:07.289 | 26/01/16 21:48:07 INFO TaskSchedulerImpl: Removed TaskSet 21.0 whose tasks have all completed, from pool 
2026-01-16 15:48:07.289 | 26/01/16 21:48:07 INFO DAGScheduler: ResultStage 21 (text at ReadWrite.scala:445) finished in 2812 ms
2026-01-16 15:48:07.290 | 26/01/16 21:48:07 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
2026-01-16 15:48:07.290 | 26/01/16 21:48:07 INFO TaskSchedulerImpl: Canceling stage 21
2026-01-16 15:48:07.290 | 26/01/16 21:48:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
2026-01-16 15:48:07.290 | 26/01/16 21:48:07 INFO DAGScheduler: Job 11 finished: text at ReadWrite.scala:445, took 2814.443934 ms
2026-01-16 15:48:07.292 | 26/01/16 21:48:07 INFO FileFormatWriter: Start to commit write Job ddb9374a-b78b-4b09-8859-50b68db947e0.
2026-01-16 15:48:07.292 | 26/01/16 21:48:07 INFO AbstractS3ACommitter: Starting: Task committer attempt_202601162148035594752130378811319_0000_m_000000_0: commitJob((no job ID))
2026-01-16 15:48:07.422 | 26/01/16 21:48:07 INFO AbstractS3ACommitter: Starting: committing the output of 1 task(s)
2026-01-16 15:48:07.424 | 26/01/16 21:48:07 INFO AbstractS3ACommitter: Starting: Loading and committing files in pendingset s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/metadata/__magic_job-ddb9374a-b78b-4b09-8859-50b68db947e0/job-ddb9374a-b78b-4b09-8859-50b68db947e0/00/task_202601162148047753393528417393414_0021_m_000000.pendingset
2026-01-16 15:48:07.707 | 26/01/16 21:48:07 INFO AbstractS3ACommitter: Loading and committing files in pendingset s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/metadata/__magic_job-ddb9374a-b78b-4b09-8859-50b68db947e0/job-ddb9374a-b78b-4b09-8859-50b68db947e0/00/task_202601162148047753393528417393414_0021_m_000000.pendingset: duration 0:00.283s
2026-01-16 15:48:07.717 | 26/01/16 21:48:07 INFO AbstractS3ACommitter: committing the output of 1 task(s): duration 0:00.296s
2026-01-16 15:48:07.722 | 26/01/16 21:48:07 INFO CommitOperations: Starting: Writing success file s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/metadata/_SUCCESS
2026-01-16 15:48:08.008 | 26/01/16 21:48:08 INFO CommitOperations: Writing success file s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/metadata/_SUCCESS: duration 0:00.283s
2026-01-16 15:48:08.008 | 26/01/16 21:48:08 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
2026-01-16 15:48:08.008 | 26/01/16 21:48:08 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/metadata
2026-01-16 15:48:08.131 | 26/01/16 21:48:08 INFO AbstractS3ACommitter: No pending uploads were found
2026-01-16 15:48:08.131 | 26/01/16 21:48:08 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/metadata: duration 0:00.121s
2026-01-16 15:48:08.131 | 26/01/16 21:48:08 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.123s
2026-01-16 15:48:08.131 | 26/01/16 21:48:08 INFO MagicS3GuardCommitter: Starting: Deleting magic directory s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/metadata/__magic_job-ddb9374a-b78b-4b09-8859-50b68db947e0
2026-01-16 15:48:09.105 | 26/01/16 21:48:09 INFO MagicS3GuardCommitter: Deleting magic directory s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/metadata/__magic_job-ddb9374a-b78b-4b09-8859-50b68db947e0: duration 0:00.975s
2026-01-16 15:48:09.105 | 26/01/16 21:48:09 INFO AbstractS3ACommitter: Task committer attempt_202601162148035594752130378811319_0000_m_000000_0: commitJob((no job ID)): duration 0:01.812s
2026-01-16 15:48:09.340 | 26/01/16 21:48:09 INFO FileFormatWriter: Write Job ddb9374a-b78b-4b09-8859-50b68db947e0 committed. Elapsed time: 2047 ms.
2026-01-16 15:48:09.342 | 26/01/16 21:48:09 INFO FileFormatWriter: Finished processing stats for write job ddb9374a-b78b-4b09-8859-50b68db947e0.
2026-01-16 15:48:09.834 | 26/01/16 21:48:09 INFO PathOutputCommitterFactory: Using schema-specific factory for s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/0_StringIndexer_a05d8ae481b9/metadata
2026-01-16 15:48:09.834 | 26/01/16 21:48:09 INFO PathOutputCommitterFactory: Using OutputCommitter factory class class org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory from key mapreduce.outputcommitter.factory.scheme.s3a
2026-01-16 15:48:09.834 | 26/01/16 21:48:09 INFO AbstractS3ACommitter: Job UUID 0d997fa1-3f39-494d-b36c-81ac64e16868 source spark.sql.sources.writeJobUUID
2026-01-16 15:48:09.835 | 26/01/16 21:48:09 INFO AbstractS3ACommitterFactory: Using committer magic to output data to s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/0_StringIndexer_a05d8ae481b9/metadata
2026-01-16 15:48:09.835 | 26/01/16 21:48:09 INFO AbstractS3ACommitterFactory: Using Committer MagicCommitter{AbstractS3ACommitter{role=Task committer attempt_202601162148091807501251544828307_0000_m_000000_0, name=magic, outputPath=s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/0_StringIndexer_a05d8ae481b9/metadata, workPath=s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/0_StringIndexer_a05d8ae481b9/metadata/__magic_job-0d997fa1-3f39-494d-b36c-81ac64e16868/job-0d997fa1-3f39-494d-b36c-81ac64e16868/00/tasks/attempt_202601162148091807501251544828307_0000_m_000000_0/__base, uuid='0d997fa1-3f39-494d-b36c-81ac64e16868', uuid source=JobUUIDSource{text='spark.sql.sources.writeJobUUID'}}} for s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/0_StringIndexer_a05d8ae481b9/metadata created by org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory@4fff95fe
2026-01-16 15:48:09.835 | 26/01/16 21:48:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter
2026-01-16 15:48:09.835 | 26/01/16 21:48:09 INFO MagicS3GuardCommitter: Starting: Setup Job (no job ID)
2026-01-16 15:48:09.836 | 26/01/16 21:48:09 INFO AbstractS3ACommitter: Starting: Job 0d997fa1-3f39-494d-b36c-81ac64e16868 setting up
2026-01-16 15:48:11.138 | 26/01/16 21:48:11 INFO AbstractS3ACommitter: Job 0d997fa1-3f39-494d-b36c-81ac64e16868 setting up: duration 0:01.295s
2026-01-16 15:48:11.405 | 26/01/16 21:48:11 INFO MagicS3GuardCommitter: Setup Job (no job ID): duration 0:01.570s
2026-01-16 15:48:11.409 | 26/01/16 21:48:11 INFO SparkContext: Starting job: text at ReadWrite.scala:445
2026-01-16 15:48:11.410 | 26/01/16 21:48:11 INFO DAGScheduler: Got job 12 (text at ReadWrite.scala:445) with 1 output partitions
2026-01-16 15:48:11.411 | 26/01/16 21:48:11 INFO DAGScheduler: Final stage: ResultStage 22 (text at ReadWrite.scala:445)
2026-01-16 15:48:11.411 | 26/01/16 21:48:11 INFO DAGScheduler: Parents of final stage: List()
2026-01-16 15:48:11.411 | 26/01/16 21:48:11 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:48:11.411 | 26/01/16 21:48:11 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[53] at text at ReadWrite.scala:445), which has no missing parents
2026-01-16 15:48:11.432 | 26/01/16 21:48:11 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 238.4 KiB, free 413.1 MiB)
2026-01-16 15:48:11.435 | 26/01/16 21:48:11 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 87.0 KiB, free 413.1 MiB)
2026-01-16 15:48:11.437 | 26/01/16 21:48:11 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:48:11.438 | 26/01/16 21:48:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[53] at text at ReadWrite.scala:445) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:48:11.438 | 26/01/16 21:48:11 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
2026-01-16 15:48:11.439 | 26/01/16 21:48:11 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 12) (10.1.4.96,executor 2, partition 0, PROCESS_LOCAL, 10222 bytes) 
2026-01-16 15:48:13.844 | 26/01/16 21:48:13 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 12) in 2406 ms on 10.1.4.96 (executor 2) (1/1)
2026-01-16 15:48:13.844 | 26/01/16 21:48:13 INFO TaskSchedulerImpl: Removed TaskSet 22.0 whose tasks have all completed, from pool 
2026-01-16 15:48:13.845 | 26/01/16 21:48:13 INFO DAGScheduler: ResultStage 22 (text at ReadWrite.scala:445) finished in 2434 ms
2026-01-16 15:48:13.845 | 26/01/16 21:48:13 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
2026-01-16 15:48:13.845 | 26/01/16 21:48:13 INFO TaskSchedulerImpl: Canceling stage 22
2026-01-16 15:48:13.845 | 26/01/16 21:48:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
2026-01-16 15:48:13.845 | 26/01/16 21:48:13 INFO DAGScheduler: Job 12 finished: text at ReadWrite.scala:445, took 2436.029836 ms
2026-01-16 15:48:13.845 | 26/01/16 21:48:13 INFO FileFormatWriter: Start to commit write Job 0d997fa1-3f39-494d-b36c-81ac64e16868.
2026-01-16 15:48:13.845 | 26/01/16 21:48:13 INFO AbstractS3ACommitter: Starting: Task committer attempt_202601162148091807501251544828307_0000_m_000000_0: commitJob((no job ID))
2026-01-16 15:48:13.968 | 26/01/16 21:48:13 INFO AbstractS3ACommitter: Starting: committing the output of 1 task(s)
2026-01-16 15:48:13.969 | 26/01/16 21:48:13 INFO AbstractS3ACommitter: Starting: Loading and committing files in pendingset s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/0_StringIndexer_a05d8ae481b9/metadata/__magic_job-0d997fa1-3f39-494d-b36c-81ac64e16868/job-0d997fa1-3f39-494d-b36c-81ac64e16868/00/task_202601162148119059175154828381058_0022_m_000000.pendingset
2026-01-16 15:48:14.102 | 26/01/16 21:48:14 INFO AbstractS3ACommitter: Loading and committing files in pendingset s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/0_StringIndexer_a05d8ae481b9/metadata/__magic_job-0d997fa1-3f39-494d-b36c-81ac64e16868/job-0d997fa1-3f39-494d-b36c-81ac64e16868/00/task_202601162148119059175154828381058_0022_m_000000.pendingset: duration 0:00.133s
2026-01-16 15:48:14.103 | 26/01/16 21:48:14 INFO AbstractS3ACommitter: committing the output of 1 task(s): duration 0:00.134s
2026-01-16 15:48:14.103 | 26/01/16 21:48:14 INFO CommitOperations: Starting: Writing success file s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/0_StringIndexer_a05d8ae481b9/metadata/_SUCCESS
2026-01-16 15:48:14.348 | 26/01/16 21:48:14 INFO CommitOperations: Writing success file s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/0_StringIndexer_a05d8ae481b9/metadata/_SUCCESS: duration 0:00.242s
2026-01-16 15:48:14.348 | 26/01/16 21:48:14 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
2026-01-16 15:48:14.348 | 26/01/16 21:48:14 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/0_StringIndexer_a05d8ae481b9/metadata
2026-01-16 15:48:14.472 | 26/01/16 21:48:14 INFO AbstractS3ACommitter: No pending uploads were found
2026-01-16 15:48:14.472 | 26/01/16 21:48:14 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/0_StringIndexer_a05d8ae481b9/metadata: duration 0:00.124s
2026-01-16 15:48:14.472 | 26/01/16 21:48:14 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.124s
2026-01-16 15:48:14.472 | 26/01/16 21:48:14 INFO MagicS3GuardCommitter: Starting: Deleting magic directory s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/0_StringIndexer_a05d8ae481b9/metadata/__magic_job-0d997fa1-3f39-494d-b36c-81ac64e16868
2026-01-16 15:48:15.428 | 26/01/16 21:48:15 INFO MagicS3GuardCommitter: Deleting magic directory s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/0_StringIndexer_a05d8ae481b9/metadata/__magic_job-0d997fa1-3f39-494d-b36c-81ac64e16868: duration 0:00.956s
2026-01-16 15:48:15.428 | 26/01/16 21:48:15 INFO AbstractS3ACommitter: Task committer attempt_202601162148091807501251544828307_0000_m_000000_0: commitJob((no job ID)): duration 0:01.580s
2026-01-16 15:48:15.679 | 26/01/16 21:48:15 INFO FileFormatWriter: Write Job 0d997fa1-3f39-494d-b36c-81ac64e16868 committed. Elapsed time: 1833 ms.
2026-01-16 15:48:15.679 | 26/01/16 21:48:15 INFO FileFormatWriter: Finished processing stats for write job 0d997fa1-3f39-494d-b36c-81ac64e16868.
2026-01-16 15:48:15.952 | 26/01/16 21:48:15 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2026-01-16 15:48:15.952 | 26/01/16 21:48:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
2026-01-16 15:48:15.952 | 26/01/16 21:48:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2026-01-16 15:48:15.952 | 26/01/16 21:48:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2026-01-16 15:48:15.952 | 26/01/16 21:48:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
2026-01-16 15:48:15.952 | 26/01/16 21:48:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2026-01-16 15:48:15.952 | 26/01/16 21:48:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2026-01-16 15:48:16.896 | 26/01/16 21:48:16 INFO CodeGenerator: Code generated in 3.372631 ms
2026-01-16 15:48:16.896 | 26/01/16 21:48:16 INFO SparkContext: Starting job: parquet at StringIndexer.scala:482
2026-01-16 15:48:16.896 | 26/01/16 21:48:16 INFO DAGScheduler: Got job 13 (parquet at StringIndexer.scala:482) with 1 output partitions
2026-01-16 15:48:16.896 | 26/01/16 21:48:16 INFO DAGScheduler: Final stage: ResultStage 23 (parquet at StringIndexer.scala:482)
2026-01-16 15:48:16.896 | 26/01/16 21:48:16 INFO DAGScheduler: Parents of final stage: List()
2026-01-16 15:48:16.896 | 26/01/16 21:48:16 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:48:16.897 | 26/01/16 21:48:16 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[56] at parquet at StringIndexer.scala:482), which has no missing parents
2026-01-16 15:48:16.905 | 26/01/16 21:48:16 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 239.1 KiB, free 412.8 MiB)
2026-01-16 15:48:16.906 | 26/01/16 21:48:16 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 87.1 KiB, free 412.7 MiB)
2026-01-16 15:48:16.907 | 26/01/16 21:48:16 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:48:16.907 | 26/01/16 21:48:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[56] at parquet at StringIndexer.scala:482) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:48:16.907 | 26/01/16 21:48:16 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
2026-01-16 15:48:16.908 | 26/01/16 21:48:16 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 13) (10.1.4.96,executor 2, partition 0, PROCESS_LOCAL, 9947 bytes) 
2026-01-16 15:48:19.622 | 26/01/16 21:48:19 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 13) in 2714 ms on 10.1.4.96 (executor 2) (1/1)
2026-01-16 15:48:19.622 | 26/01/16 21:48:19 INFO TaskSchedulerImpl: Removed TaskSet 23.0 whose tasks have all completed, from pool 
2026-01-16 15:48:19.623 | 26/01/16 21:48:19 INFO DAGScheduler: ResultStage 23 (parquet at StringIndexer.scala:482) finished in 2726 ms
2026-01-16 15:48:19.623 | 26/01/16 21:48:19 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
2026-01-16 15:48:19.623 | 26/01/16 21:48:19 INFO TaskSchedulerImpl: Canceling stage 23
2026-01-16 15:48:19.623 | 26/01/16 21:48:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
2026-01-16 15:48:19.623 | 26/01/16 21:48:19 INFO DAGScheduler: Job 13 finished: parquet at StringIndexer.scala:482, took 2731.718713 ms
2026-01-16 15:48:19.623 | 26/01/16 21:48:19 INFO FileFormatWriter: Start to commit write Job 4c71e830-3bd0-4dea-b18d-3bdd41a40e67.
2026-01-16 15:48:22.899 | 26/01/16 21:48:22 INFO FileFormatWriter: Write Job 4c71e830-3bd0-4dea-b18d-3bdd41a40e67 committed. Elapsed time: 3274 ms.
2026-01-16 15:48:22.899 | 26/01/16 21:48:22 INFO FileFormatWriter: Finished processing stats for write job 4c71e830-3bd0-4dea-b18d-3bdd41a40e67.
2026-01-16 15:48:23.378 | 26/01/16 21:48:23 INFO PathOutputCommitterFactory: Using schema-specific factory for s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/1_StringIndexer_b769b0cfb063/metadata
2026-01-16 15:48:23.378 | 26/01/16 21:48:23 INFO PathOutputCommitterFactory: Using OutputCommitter factory class class org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory from key mapreduce.outputcommitter.factory.scheme.s3a
2026-01-16 15:48:23.379 | 26/01/16 21:48:23 INFO AbstractS3ACommitter: Job UUID 9bc76423-e738-4e39-839b-d082e34df82c source spark.sql.sources.writeJobUUID
2026-01-16 15:48:23.379 | 26/01/16 21:48:23 INFO AbstractS3ACommitterFactory: Using committer magic to output data to s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/1_StringIndexer_b769b0cfb063/metadata
2026-01-16 15:48:23.379 | 26/01/16 21:48:23 INFO AbstractS3ACommitterFactory: Using Committer MagicCommitter{AbstractS3ACommitter{role=Task committer attempt_202601162148232727884920427615398_0000_m_000000_0, name=magic, outputPath=s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/1_StringIndexer_b769b0cfb063/metadata, workPath=s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/1_StringIndexer_b769b0cfb063/metadata/__magic_job-9bc76423-e738-4e39-839b-d082e34df82c/job-9bc76423-e738-4e39-839b-d082e34df82c/00/tasks/attempt_202601162148232727884920427615398_0000_m_000000_0/__base, uuid='9bc76423-e738-4e39-839b-d082e34df82c', uuid source=JobUUIDSource{text='spark.sql.sources.writeJobUUID'}}} for s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/1_StringIndexer_b769b0cfb063/metadata created by org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory@3c0d9a44
2026-01-16 15:48:23.379 | 26/01/16 21:48:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter
2026-01-16 15:48:23.379 | 26/01/16 21:48:23 INFO MagicS3GuardCommitter: Starting: Setup Job (no job ID)
2026-01-16 15:48:23.379 | 26/01/16 21:48:23 INFO AbstractS3ACommitter: Starting: Job 9bc76423-e738-4e39-839b-d082e34df82c setting up
2026-01-16 15:48:24.432 | 26/01/16 21:48:24 INFO AbstractS3ACommitter: Job 9bc76423-e738-4e39-839b-d082e34df82c setting up: duration 0:01.052s
2026-01-16 15:48:24.674 | 26/01/16 21:48:24 INFO MagicS3GuardCommitter: Setup Job (no job ID): duration 0:01.290s
2026-01-16 15:48:24.674 | 26/01/16 21:48:24 INFO SparkContext: Starting job: text at ReadWrite.scala:445
2026-01-16 15:48:24.675 | 26/01/16 21:48:24 INFO DAGScheduler: Got job 14 (text at ReadWrite.scala:445) with 1 output partitions
2026-01-16 15:48:24.675 | 26/01/16 21:48:24 INFO DAGScheduler: Final stage: ResultStage 24 (text at ReadWrite.scala:445)
2026-01-16 15:48:24.675 | 26/01/16 21:48:24 INFO DAGScheduler: Parents of final stage: List()
2026-01-16 15:48:24.675 | 26/01/16 21:48:24 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:48:24.675 | 26/01/16 21:48:24 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[59] at text at ReadWrite.scala:445), which has no missing parents
2026-01-16 15:48:24.682 | 26/01/16 21:48:24 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 238.4 KiB, free 412.5 MiB)
2026-01-16 15:48:24.693 | 26/01/16 21:48:24 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 87.0 KiB, free 412.4 MiB)
2026-01-16 15:48:24.694 | 26/01/16 21:48:24 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:48:24.694 | 26/01/16 21:48:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[59] at text at ReadWrite.scala:445) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:48:24.694 | 26/01/16 21:48:24 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
2026-01-16 15:48:24.695 | 26/01/16 21:48:24 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 14) (10.1.4.96,executor 2, partition 0, PROCESS_LOCAL, 10222 bytes) 
2026-01-16 15:48:27.011 | 26/01/16 21:48:27 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 14) in 2313 ms on 10.1.4.96 (executor 2) (1/1)
2026-01-16 15:48:27.011 | 26/01/16 21:48:27 INFO TaskSchedulerImpl: Removed TaskSet 24.0 whose tasks have all completed, from pool 
2026-01-16 15:48:27.012 | 26/01/16 21:48:27 INFO DAGScheduler: ResultStage 24 (text at ReadWrite.scala:445) finished in 2337 ms
2026-01-16 15:48:27.012 | 26/01/16 21:48:27 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
2026-01-16 15:48:27.012 | 26/01/16 21:48:27 INFO TaskSchedulerImpl: Canceling stage 24
2026-01-16 15:48:27.012 | 26/01/16 21:48:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
2026-01-16 15:48:27.012 | 26/01/16 21:48:27 INFO DAGScheduler: Job 14 finished: text at ReadWrite.scala:445, took 2339.985574 ms
2026-01-16 15:48:27.012 | 26/01/16 21:48:27 INFO FileFormatWriter: Start to commit write Job 9bc76423-e738-4e39-839b-d082e34df82c.
2026-01-16 15:48:27.012 | 26/01/16 21:48:27 INFO AbstractS3ACommitter: Starting: Task committer attempt_202601162148232727884920427615398_0000_m_000000_0: commitJob((no job ID))
2026-01-16 15:48:27.279 | 26/01/16 21:48:27 INFO AbstractS3ACommitter: Starting: committing the output of 1 task(s)
2026-01-16 15:48:27.280 | 26/01/16 21:48:27 INFO AbstractS3ACommitter: Starting: Loading and committing files in pendingset s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/1_StringIndexer_b769b0cfb063/metadata/__magic_job-9bc76423-e738-4e39-839b-d082e34df82c/job-9bc76423-e738-4e39-839b-d082e34df82c/00/task_202601162148244493308769892178571_0024_m_000000.pendingset
2026-01-16 15:48:27.416 | 26/01/16 21:48:27 INFO AbstractS3ACommitter: Loading and committing files in pendingset s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/1_StringIndexer_b769b0cfb063/metadata/__magic_job-9bc76423-e738-4e39-839b-d082e34df82c/job-9bc76423-e738-4e39-839b-d082e34df82c/00/task_202601162148244493308769892178571_0024_m_000000.pendingset: duration 0:00.136s
2026-01-16 15:48:27.421 | 26/01/16 21:48:27 INFO AbstractS3ACommitter: committing the output of 1 task(s): duration 0:00.142s
2026-01-16 15:48:27.422 | 26/01/16 21:48:27 INFO CommitOperations: Starting: Writing success file s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/1_StringIndexer_b769b0cfb063/metadata/_SUCCESS
2026-01-16 15:48:27.662 | 26/01/16 21:48:27 INFO CommitOperations: Writing success file s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/1_StringIndexer_b769b0cfb063/metadata/_SUCCESS: duration 0:00.239s
2026-01-16 15:48:27.662 | 26/01/16 21:48:27 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
2026-01-16 15:48:27.662 | 26/01/16 21:48:27 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/1_StringIndexer_b769b0cfb063/metadata
2026-01-16 15:48:27.780 | 26/01/16 21:48:27 INFO AbstractS3ACommitter: No pending uploads were found
2026-01-16 15:48:27.780 | 26/01/16 21:48:27 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/1_StringIndexer_b769b0cfb063/metadata: duration 0:00.117s
2026-01-16 15:48:27.780 | 26/01/16 21:48:27 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.117s
2026-01-16 15:48:27.780 | 26/01/16 21:48:27 INFO MagicS3GuardCommitter: Starting: Deleting magic directory s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/1_StringIndexer_b769b0cfb063/metadata/__magic_job-9bc76423-e738-4e39-839b-d082e34df82c
2026-01-16 15:48:28.815 | 26/01/16 21:48:28 INFO MagicS3GuardCommitter: Deleting magic directory s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/1_StringIndexer_b769b0cfb063/metadata/__magic_job-9bc76423-e738-4e39-839b-d082e34df82c: duration 0:01.035s
2026-01-16 15:48:28.815 | 26/01/16 21:48:28 INFO AbstractS3ACommitter: Task committer attempt_202601162148232727884920427615398_0000_m_000000_0: commitJob((no job ID)): duration 0:01.802s
2026-01-16 15:48:29.045 | 26/01/16 21:48:29 INFO FileFormatWriter: Write Job 9bc76423-e738-4e39-839b-d082e34df82c committed. Elapsed time: 1892 ms.
2026-01-16 15:48:29.046 | 26/01/16 21:48:29 INFO FileFormatWriter: Finished processing stats for write job 9bc76423-e738-4e39-839b-d082e34df82c.
2026-01-16 15:48:29.327 | 26/01/16 21:48:29 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2026-01-16 15:48:29.327 | 26/01/16 21:48:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
2026-01-16 15:48:29.327 | 26/01/16 21:48:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2026-01-16 15:48:29.327 | 26/01/16 21:48:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2026-01-16 15:48:29.327 | 26/01/16 21:48:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
2026-01-16 15:48:29.327 | 26/01/16 21:48:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2026-01-16 15:48:29.327 | 26/01/16 21:48:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2026-01-16 15:48:30.291 | 26/01/16 21:48:30 INFO SparkContext: Starting job: parquet at StringIndexer.scala:482
2026-01-16 15:48:30.291 | 26/01/16 21:48:30 INFO DAGScheduler: Got job 15 (parquet at StringIndexer.scala:482) with 1 output partitions
2026-01-16 15:48:30.291 | 26/01/16 21:48:30 INFO DAGScheduler: Final stage: ResultStage 25 (parquet at StringIndexer.scala:482)
2026-01-16 15:48:30.291 | 26/01/16 21:48:30 INFO DAGScheduler: Parents of final stage: List()
2026-01-16 15:48:30.291 | 26/01/16 21:48:30 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:48:30.292 | 26/01/16 21:48:30 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[62] at parquet at StringIndexer.scala:482), which has no missing parents
2026-01-16 15:48:30.300 | 26/01/16 21:48:30 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 239.1 KiB, free 412.8 MiB)
2026-01-16 15:48:30.303 | 26/01/16 21:48:30 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 87.1 KiB, free 412.7 MiB)
2026-01-16 15:48:30.307 | 26/01/16 21:48:30 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:48:30.307 | 26/01/16 21:48:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[62] at parquet at StringIndexer.scala:482) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:48:30.307 | 26/01/16 21:48:30 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
2026-01-16 15:48:30.308 | 26/01/16 21:48:30 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 15) (10.1.4.95,executor 1, partition 0, PROCESS_LOCAL, 9931 bytes) 
2026-01-16 15:48:33.163 | 26/01/16 21:48:33 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 15) in 2855 ms on 10.1.4.95 (executor 1) (1/1)
2026-01-16 15:48:33.163 | 26/01/16 21:48:33 INFO TaskSchedulerImpl: Removed TaskSet 25.0 whose tasks have all completed, from pool 
2026-01-16 15:48:33.164 | 26/01/16 21:48:33 INFO DAGScheduler: ResultStage 25 (parquet at StringIndexer.scala:482) finished in 2871 ms
2026-01-16 15:48:33.164 | 26/01/16 21:48:33 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
2026-01-16 15:48:33.164 | 26/01/16 21:48:33 INFO TaskSchedulerImpl: Canceling stage 25
2026-01-16 15:48:33.164 | 26/01/16 21:48:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
2026-01-16 15:48:33.164 | 26/01/16 21:48:33 INFO DAGScheduler: Job 15 finished: parquet at StringIndexer.scala:482, took 2875.485623 ms
2026-01-16 15:48:33.164 | 26/01/16 21:48:33 INFO FileFormatWriter: Start to commit write Job f7e7a036-efb9-4a7b-a556-5068fe3a63fd.
2026-01-16 15:48:36.368 | 26/01/16 21:48:36 INFO FileFormatWriter: Write Job f7e7a036-efb9-4a7b-a556-5068fe3a63fd committed. Elapsed time: 3203 ms.
2026-01-16 15:48:36.369 | 26/01/16 21:48:36 INFO FileFormatWriter: Finished processing stats for write job f7e7a036-efb9-4a7b-a556-5068fe3a63fd.
2026-01-16 15:48:36.844 | 26/01/16 21:48:36 INFO PathOutputCommitterFactory: Using schema-specific factory for s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/2_StringIndexer_88708e6ed528/metadata
2026-01-16 15:48:36.844 | 26/01/16 21:48:36 INFO PathOutputCommitterFactory: Using OutputCommitter factory class class org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory from key mapreduce.outputcommitter.factory.scheme.s3a
2026-01-16 15:48:36.844 | 26/01/16 21:48:36 INFO AbstractS3ACommitter: Job UUID db3ce1a6-6cf4-4bbc-b224-cd5f57033d42 source spark.sql.sources.writeJobUUID
2026-01-16 15:48:36.844 | 26/01/16 21:48:36 INFO AbstractS3ACommitterFactory: Using committer magic to output data to s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/2_StringIndexer_88708e6ed528/metadata
2026-01-16 15:48:36.844 | 26/01/16 21:48:36 INFO AbstractS3ACommitterFactory: Using Committer MagicCommitter{AbstractS3ACommitter{role=Task committer attempt_202601162148367740479753896642484_0000_m_000000_0, name=magic, outputPath=s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/2_StringIndexer_88708e6ed528/metadata, workPath=s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/2_StringIndexer_88708e6ed528/metadata/__magic_job-db3ce1a6-6cf4-4bbc-b224-cd5f57033d42/job-db3ce1a6-6cf4-4bbc-b224-cd5f57033d42/00/tasks/attempt_202601162148367740479753896642484_0000_m_000000_0/__base, uuid='db3ce1a6-6cf4-4bbc-b224-cd5f57033d42', uuid source=JobUUIDSource{text='spark.sql.sources.writeJobUUID'}}} for s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/2_StringIndexer_88708e6ed528/metadata created by org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory@681f3466
2026-01-16 15:48:36.844 | 26/01/16 21:48:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter
2026-01-16 15:48:36.844 | 26/01/16 21:48:36 INFO MagicS3GuardCommitter: Starting: Setup Job (no job ID)
2026-01-16 15:48:36.844 | 26/01/16 21:48:36 INFO AbstractS3ACommitter: Starting: Job db3ce1a6-6cf4-4bbc-b224-cd5f57033d42 setting up
2026-01-16 15:48:37.909 | 26/01/16 21:48:37 INFO AbstractS3ACommitter: Job db3ce1a6-6cf4-4bbc-b224-cd5f57033d42 setting up: duration 0:01.069s
2026-01-16 15:48:38.151 | 26/01/16 21:48:38 INFO MagicS3GuardCommitter: Setup Job (no job ID): duration 0:01.310s
2026-01-16 15:48:38.154 | 26/01/16 21:48:38 INFO SparkContext: Starting job: text at ReadWrite.scala:445
2026-01-16 15:48:38.155 | 26/01/16 21:48:38 INFO DAGScheduler: Got job 16 (text at ReadWrite.scala:445) with 1 output partitions
2026-01-16 15:48:38.155 | 26/01/16 21:48:38 INFO DAGScheduler: Final stage: ResultStage 26 (text at ReadWrite.scala:445)
2026-01-16 15:48:38.156 | 26/01/16 21:48:38 INFO DAGScheduler: Parents of final stage: List()
2026-01-16 15:48:38.156 | 26/01/16 21:48:38 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:48:38.156 | 26/01/16 21:48:38 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[65] at text at ReadWrite.scala:445), which has no missing parents
2026-01-16 15:48:38.163 | 26/01/16 21:48:38 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 238.4 KiB, free 412.5 MiB)
2026-01-16 15:48:38.164 | 26/01/16 21:48:38 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 87.0 KiB, free 412.4 MiB)
2026-01-16 15:48:38.165 | 26/01/16 21:48:38 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:48:38.166 | 26/01/16 21:48:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[65] at text at ReadWrite.scala:445) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:48:38.166 | 26/01/16 21:48:38 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
2026-01-16 15:48:38.169 | 26/01/16 21:48:38 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 16) (10.1.4.96,executor 2, partition 0, PROCESS_LOCAL, 10230 bytes) 
2026-01-16 15:48:40.454 | 26/01/16 21:48:40 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 16) in 2286 ms on 10.1.4.96 (executor 2) (1/1)
2026-01-16 15:48:40.454 | 26/01/16 21:48:40 INFO TaskSchedulerImpl: Removed TaskSet 26.0 whose tasks have all completed, from pool 
2026-01-16 15:48:40.454 | 26/01/16 21:48:40 INFO DAGScheduler: ResultStage 26 (text at ReadWrite.scala:445) finished in 2299 ms
2026-01-16 15:48:40.454 | 26/01/16 21:48:40 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
2026-01-16 15:48:40.454 | 26/01/16 21:48:40 INFO TaskSchedulerImpl: Canceling stage 26
2026-01-16 15:48:40.454 | 26/01/16 21:48:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
2026-01-16 15:48:40.455 | 26/01/16 21:48:40 INFO DAGScheduler: Job 16 finished: text at ReadWrite.scala:445, took 2299.940334 ms
2026-01-16 15:48:40.455 | 26/01/16 21:48:40 INFO FileFormatWriter: Start to commit write Job db3ce1a6-6cf4-4bbc-b224-cd5f57033d42.
2026-01-16 15:48:40.455 | 26/01/16 21:48:40 INFO AbstractS3ACommitter: Starting: Task committer attempt_202601162148367740479753896642484_0000_m_000000_0: commitJob((no job ID))
2026-01-16 15:48:40.716 | 26/01/16 21:48:40 INFO AbstractS3ACommitter: Starting: committing the output of 1 task(s)
2026-01-16 15:48:40.721 | 26/01/16 21:48:40 INFO AbstractS3ACommitter: Starting: Loading and committing files in pendingset s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/2_StringIndexer_88708e6ed528/metadata/__magic_job-db3ce1a6-6cf4-4bbc-b224-cd5f57033d42/job-db3ce1a6-6cf4-4bbc-b224-cd5f57033d42/00/task_202601162148388417110873492478460_0026_m_000000.pendingset
2026-01-16 15:48:40.857 | 26/01/16 21:48:40 INFO AbstractS3ACommitter: Loading and committing files in pendingset s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/2_StringIndexer_88708e6ed528/metadata/__magic_job-db3ce1a6-6cf4-4bbc-b224-cd5f57033d42/job-db3ce1a6-6cf4-4bbc-b224-cd5f57033d42/00/task_202601162148388417110873492478460_0026_m_000000.pendingset: duration 0:00.137s
2026-01-16 15:48:40.858 | 26/01/16 21:48:40 INFO AbstractS3ACommitter: committing the output of 1 task(s): duration 0:00.142s
2026-01-16 15:48:40.859 | 26/01/16 21:48:40 INFO CommitOperations: Starting: Writing success file s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/2_StringIndexer_88708e6ed528/metadata/_SUCCESS
2026-01-16 15:48:41.102 | 26/01/16 21:48:41 INFO CommitOperations: Writing success file s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/2_StringIndexer_88708e6ed528/metadata/_SUCCESS: duration 0:00.242s
2026-01-16 15:48:41.102 | 26/01/16 21:48:41 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
2026-01-16 15:48:41.102 | 26/01/16 21:48:41 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/2_StringIndexer_88708e6ed528/metadata
2026-01-16 15:48:41.219 | 26/01/16 21:48:41 INFO AbstractS3ACommitter: No pending uploads were found
2026-01-16 15:48:41.220 | 26/01/16 21:48:41 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/2_StringIndexer_88708e6ed528/metadata: duration 0:00.118s
2026-01-16 15:48:41.220 | 26/01/16 21:48:41 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.118s
2026-01-16 15:48:41.220 | 26/01/16 21:48:41 INFO MagicS3GuardCommitter: Starting: Deleting magic directory s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/2_StringIndexer_88708e6ed528/metadata/__magic_job-db3ce1a6-6cf4-4bbc-b224-cd5f57033d42
2026-01-16 15:48:42.189 | 26/01/16 21:48:42 INFO MagicS3GuardCommitter: Deleting magic directory s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/2_StringIndexer_88708e6ed528/metadata/__magic_job-db3ce1a6-6cf4-4bbc-b224-cd5f57033d42: duration 0:00.970s
2026-01-16 15:48:42.189 | 26/01/16 21:48:42 INFO AbstractS3ACommitter: Task committer attempt_202601162148367740479753896642484_0000_m_000000_0: commitJob((no job ID)): duration 0:01.734s
2026-01-16 15:48:42.429 | 26/01/16 21:48:42 INFO FileFormatWriter: Write Job db3ce1a6-6cf4-4bbc-b224-cd5f57033d42 committed. Elapsed time: 1974 ms.
2026-01-16 15:48:42.430 | 26/01/16 21:48:42 INFO FileFormatWriter: Finished processing stats for write job db3ce1a6-6cf4-4bbc-b224-cd5f57033d42.
2026-01-16 15:48:42.676 | 26/01/16 21:48:42 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2026-01-16 15:48:42.678 | 26/01/16 21:48:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
2026-01-16 15:48:42.678 | 26/01/16 21:48:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2026-01-16 15:48:42.678 | 26/01/16 21:48:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2026-01-16 15:48:42.678 | 26/01/16 21:48:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
2026-01-16 15:48:42.678 | 26/01/16 21:48:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2026-01-16 15:48:42.679 | 26/01/16 21:48:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2026-01-16 15:48:43.615 | 26/01/16 21:48:43 INFO SparkContext: Starting job: parquet at StringIndexer.scala:482
2026-01-16 15:48:43.615 | 26/01/16 21:48:43 INFO DAGScheduler: Got job 17 (parquet at StringIndexer.scala:482) with 1 output partitions
2026-01-16 15:48:43.615 | 26/01/16 21:48:43 INFO DAGScheduler: Final stage: ResultStage 27 (parquet at StringIndexer.scala:482)
2026-01-16 15:48:43.615 | 26/01/16 21:48:43 INFO DAGScheduler: Parents of final stage: List()
2026-01-16 15:48:43.615 | 26/01/16 21:48:43 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:48:43.615 | 26/01/16 21:48:43 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[68] at parquet at StringIndexer.scala:482), which has no missing parents
2026-01-16 15:48:43.627 | 26/01/16 21:48:43 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 239.1 KiB, free 412.8 MiB)
2026-01-16 15:48:43.629 | 26/01/16 21:48:43 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 87.1 KiB, free 413.1 MiB)
2026-01-16 15:48:43.630 | 26/01/16 21:48:43 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:48:43.630 | 26/01/16 21:48:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[68] at parquet at StringIndexer.scala:482) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:48:43.630 | 26/01/16 21:48:43 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
2026-01-16 15:48:43.631 | 26/01/16 21:48:43 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 17) (10.1.4.96,executor 2, partition 0, PROCESS_LOCAL, 9979 bytes) 
2026-01-16 15:48:46.688 | 26/01/16 21:48:46 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 17) in 3057 ms on 10.1.4.96 (executor 2) (1/1)
2026-01-16 15:48:46.688 | 26/01/16 21:48:46 INFO TaskSchedulerImpl: Removed TaskSet 27.0 whose tasks have all completed, from pool 
2026-01-16 15:48:46.689 | 26/01/16 21:48:46 INFO DAGScheduler: ResultStage 27 (parquet at StringIndexer.scala:482) finished in 3073 ms
2026-01-16 15:48:46.689 | 26/01/16 21:48:46 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
2026-01-16 15:48:46.689 | 26/01/16 21:48:46 INFO TaskSchedulerImpl: Canceling stage 27
2026-01-16 15:48:46.689 | 26/01/16 21:48:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
2026-01-16 15:48:46.690 | 26/01/16 21:48:46 INFO DAGScheduler: Job 17 finished: parquet at StringIndexer.scala:482, took 3074.357444 ms
2026-01-16 15:48:46.690 | 26/01/16 21:48:46 INFO FileFormatWriter: Start to commit write Job f98e4947-75ff-48fe-a265-0b10957ed8e4.
2026-01-16 15:48:49.844 | 26/01/16 21:48:49 INFO FileFormatWriter: Write Job f98e4947-75ff-48fe-a265-0b10957ed8e4 committed. Elapsed time: 3149 ms.
2026-01-16 15:48:49.844 | 26/01/16 21:48:49 INFO FileFormatWriter: Finished processing stats for write job f98e4947-75ff-48fe-a265-0b10957ed8e4.
2026-01-16 15:48:50.326 | 26/01/16 21:48:50 INFO PathOutputCommitterFactory: Using schema-specific factory for s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/3_VectorAssembler_d008ee6e79e5/metadata
2026-01-16 15:48:50.326 | 26/01/16 21:48:50 INFO PathOutputCommitterFactory: Using OutputCommitter factory class class org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory from key mapreduce.outputcommitter.factory.scheme.s3a
2026-01-16 15:48:50.326 | 26/01/16 21:48:50 INFO AbstractS3ACommitter: Job UUID 696c6b76-ae7b-4ba2-9a41-3ea6018f7ea1 source spark.sql.sources.writeJobUUID
2026-01-16 15:48:50.329 | 26/01/16 21:48:50 INFO AbstractS3ACommitterFactory: Using committer magic to output data to s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/3_VectorAssembler_d008ee6e79e5/metadata
2026-01-16 15:48:50.329 | 26/01/16 21:48:50 INFO AbstractS3ACommitterFactory: Using Committer MagicCommitter{AbstractS3ACommitter{role=Task committer attempt_202601162148507180635287274865808_0000_m_000000_0, name=magic, outputPath=s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/3_VectorAssembler_d008ee6e79e5/metadata, workPath=s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/3_VectorAssembler_d008ee6e79e5/metadata/__magic_job-696c6b76-ae7b-4ba2-9a41-3ea6018f7ea1/job-696c6b76-ae7b-4ba2-9a41-3ea6018f7ea1/00/tasks/attempt_202601162148507180635287274865808_0000_m_000000_0/__base, uuid='696c6b76-ae7b-4ba2-9a41-3ea6018f7ea1', uuid source=JobUUIDSource{text='spark.sql.sources.writeJobUUID'}}} for s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/3_VectorAssembler_d008ee6e79e5/metadata created by org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory@403b87ce
2026-01-16 15:48:50.329 | 26/01/16 21:48:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter
2026-01-16 15:48:50.329 | 26/01/16 21:48:50 INFO MagicS3GuardCommitter: Starting: Setup Job (no job ID)
2026-01-16 15:48:50.329 | 26/01/16 21:48:50 INFO AbstractS3ACommitter: Starting: Job 696c6b76-ae7b-4ba2-9a41-3ea6018f7ea1 setting up
2026-01-16 15:48:51.390 | 26/01/16 21:48:51 INFO AbstractS3ACommitter: Job 696c6b76-ae7b-4ba2-9a41-3ea6018f7ea1 setting up: duration 0:01.063s
2026-01-16 15:48:51.665 | 26/01/16 21:48:51 INFO MagicS3GuardCommitter: Setup Job (no job ID): duration 0:01.326s
2026-01-16 15:48:51.665 | 26/01/16 21:48:51 INFO SparkContext: Starting job: text at ReadWrite.scala:445
2026-01-16 15:48:51.665 | 26/01/16 21:48:51 INFO DAGScheduler: Got job 18 (text at ReadWrite.scala:445) with 1 output partitions
2026-01-16 15:48:51.665 | 26/01/16 21:48:51 INFO DAGScheduler: Final stage: ResultStage 28 (text at ReadWrite.scala:445)
2026-01-16 15:48:51.665 | 26/01/16 21:48:51 INFO DAGScheduler: Parents of final stage: List()
2026-01-16 15:48:51.665 | 26/01/16 21:48:51 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:48:51.665 | 26/01/16 21:48:51 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[71] at text at ReadWrite.scala:445), which has no missing parents
2026-01-16 15:48:51.665 | 26/01/16 21:48:51 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 238.4 KiB, free 412.8 MiB)
2026-01-16 15:48:51.665 | 26/01/16 21:48:51 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 87.0 KiB, free 412.7 MiB)
2026-01-16 15:48:51.666 | 26/01/16 21:48:51 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:48:51.667 | 26/01/16 21:48:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[71] at text at ReadWrite.scala:445) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:48:51.667 | 26/01/16 21:48:51 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0
2026-01-16 15:48:51.667 | 26/01/16 21:48:51 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 18) (10.1.4.95,executor 1, partition 0, PROCESS_LOCAL, 10342 bytes) 
2026-01-16 15:48:53.819 | 26/01/16 21:48:53 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 18) in 2151 ms on 10.1.4.95 (executor 1) (1/1)
2026-01-16 15:48:53.819 | 26/01/16 21:48:53 INFO TaskSchedulerImpl: Removed TaskSet 28.0 whose tasks have all completed, from pool 
2026-01-16 15:48:53.819 | 26/01/16 21:48:53 INFO DAGScheduler: ResultStage 28 (text at ReadWrite.scala:445) finished in 2163 ms
2026-01-16 15:48:53.819 | 26/01/16 21:48:53 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
2026-01-16 15:48:53.819 | 26/01/16 21:48:53 INFO TaskSchedulerImpl: Canceling stage 28
2026-01-16 15:48:53.819 | 26/01/16 21:48:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
2026-01-16 15:48:53.820 | 26/01/16 21:48:53 INFO DAGScheduler: Job 18 finished: text at ReadWrite.scala:445, took 2163.804692 ms
2026-01-16 15:48:53.820 | 26/01/16 21:48:53 INFO FileFormatWriter: Start to commit write Job 696c6b76-ae7b-4ba2-9a41-3ea6018f7ea1.
2026-01-16 15:48:53.820 | 26/01/16 21:48:53 INFO AbstractS3ACommitter: Starting: Task committer attempt_202601162148507180635287274865808_0000_m_000000_0: commitJob((no job ID))
2026-01-16 15:48:53.943 | 26/01/16 21:48:53 INFO AbstractS3ACommitter: Starting: committing the output of 1 task(s)
2026-01-16 15:48:53.945 | 26/01/16 21:48:53 INFO AbstractS3ACommitter: Starting: Loading and committing files in pendingset s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/3_VectorAssembler_d008ee6e79e5/metadata/__magic_job-696c6b76-ae7b-4ba2-9a41-3ea6018f7ea1/job-696c6b76-ae7b-4ba2-9a41-3ea6018f7ea1/00/task_202601162148519164022199642703977_0028_m_000000.pendingset
2026-01-16 15:48:54.075 | 26/01/16 21:48:54 INFO AbstractS3ACommitter: Loading and committing files in pendingset s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/3_VectorAssembler_d008ee6e79e5/metadata/__magic_job-696c6b76-ae7b-4ba2-9a41-3ea6018f7ea1/job-696c6b76-ae7b-4ba2-9a41-3ea6018f7ea1/00/task_202601162148519164022199642703977_0028_m_000000.pendingset: duration 0:00.131s
2026-01-16 15:48:54.077 | 26/01/16 21:48:54 INFO AbstractS3ACommitter: committing the output of 1 task(s): duration 0:00.133s
2026-01-16 15:48:54.077 | 26/01/16 21:48:54 INFO CommitOperations: Starting: Writing success file s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/3_VectorAssembler_d008ee6e79e5/metadata/_SUCCESS
2026-01-16 15:48:54.316 | 26/01/16 21:48:54 INFO CommitOperations: Writing success file s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/3_VectorAssembler_d008ee6e79e5/metadata/_SUCCESS: duration 0:00.239s
2026-01-16 15:48:54.316 | 26/01/16 21:48:54 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
2026-01-16 15:48:54.316 | 26/01/16 21:48:54 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/3_VectorAssembler_d008ee6e79e5/metadata
2026-01-16 15:48:54.440 | 26/01/16 21:48:54 INFO AbstractS3ACommitter: No pending uploads were found
2026-01-16 15:48:54.440 | 26/01/16 21:48:54 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/3_VectorAssembler_d008ee6e79e5/metadata: duration 0:00.124s
2026-01-16 15:48:54.440 | 26/01/16 21:48:54 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.124s
2026-01-16 15:48:54.440 | 26/01/16 21:48:54 INFO MagicS3GuardCommitter: Starting: Deleting magic directory s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/3_VectorAssembler_d008ee6e79e5/metadata/__magic_job-696c6b76-ae7b-4ba2-9a41-3ea6018f7ea1
2026-01-16 15:48:55.420 | 26/01/16 21:48:55 INFO MagicS3GuardCommitter: Deleting magic directory s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/3_VectorAssembler_d008ee6e79e5/metadata/__magic_job-696c6b76-ae7b-4ba2-9a41-3ea6018f7ea1: duration 0:00.981s
2026-01-16 15:48:55.420 | 26/01/16 21:48:55 INFO AbstractS3ACommitter: Task committer attempt_202601162148507180635287274865808_0000_m_000000_0: commitJob((no job ID)): duration 0:01.601s
2026-01-16 15:48:55.645 | 26/01/16 21:48:55 INFO FileFormatWriter: Write Job 696c6b76-ae7b-4ba2-9a41-3ea6018f7ea1 committed. Elapsed time: 1825 ms.
2026-01-16 15:48:55.646 | 26/01/16 21:48:55 INFO FileFormatWriter: Finished processing stats for write job 696c6b76-ae7b-4ba2-9a41-3ea6018f7ea1.
2026-01-16 15:48:56.113 | 26/01/16 21:48:56 INFO PathOutputCommitterFactory: Using schema-specific factory for s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/4_StandardScaler_64a27f836f1c/metadata
2026-01-16 15:48:56.113 | 26/01/16 21:48:56 INFO PathOutputCommitterFactory: Using OutputCommitter factory class class org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory from key mapreduce.outputcommitter.factory.scheme.s3a
2026-01-16 15:48:56.113 | 26/01/16 21:48:56 INFO AbstractS3ACommitter: Job UUID bc842b15-98db-46e4-b888-f3a69ed055ed source spark.sql.sources.writeJobUUID
2026-01-16 15:48:56.114 | 26/01/16 21:48:56 INFO AbstractS3ACommitterFactory: Using committer magic to output data to s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/4_StandardScaler_64a27f836f1c/metadata
2026-01-16 15:48:56.114 | 26/01/16 21:48:56 INFO AbstractS3ACommitterFactory: Using Committer MagicCommitter{AbstractS3ACommitter{role=Task committer attempt_202601162148565414995386474839373_0000_m_000000_0, name=magic, outputPath=s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/4_StandardScaler_64a27f836f1c/metadata, workPath=s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/4_StandardScaler_64a27f836f1c/metadata/__magic_job-bc842b15-98db-46e4-b888-f3a69ed055ed/job-bc842b15-98db-46e4-b888-f3a69ed055ed/00/tasks/attempt_202601162148565414995386474839373_0000_m_000000_0/__base, uuid='bc842b15-98db-46e4-b888-f3a69ed055ed', uuid source=JobUUIDSource{text='spark.sql.sources.writeJobUUID'}}} for s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/4_StandardScaler_64a27f836f1c/metadata created by org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory@1642d6ab
2026-01-16 15:48:56.114 | 26/01/16 21:48:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter
2026-01-16 15:48:56.114 | 26/01/16 21:48:56 INFO MagicS3GuardCommitter: Starting: Setup Job (no job ID)
2026-01-16 15:48:56.114 | 26/01/16 21:48:56 INFO AbstractS3ACommitter: Starting: Job bc842b15-98db-46e4-b888-f3a69ed055ed setting up
2026-01-16 15:48:57.154 | 26/01/16 21:48:57 INFO AbstractS3ACommitter: Job bc842b15-98db-46e4-b888-f3a69ed055ed setting up: duration 0:01.040s
2026-01-16 15:48:57.520 | 26/01/16 21:48:57 INFO MagicS3GuardCommitter: Setup Job (no job ID): duration 0:01.405s
2026-01-16 15:48:57.522 | 26/01/16 21:48:57 INFO SparkContext: Starting job: text at ReadWrite.scala:445
2026-01-16 15:48:57.522 | 26/01/16 21:48:57 INFO DAGScheduler: Got job 19 (text at ReadWrite.scala:445) with 1 output partitions
2026-01-16 15:48:57.522 | 26/01/16 21:48:57 INFO DAGScheduler: Final stage: ResultStage 29 (text at ReadWrite.scala:445)
2026-01-16 15:48:57.522 | 26/01/16 21:48:57 INFO DAGScheduler: Parents of final stage: List()
2026-01-16 15:48:57.522 | 26/01/16 21:48:57 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:48:57.522 | 26/01/16 21:48:57 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[74] at text at ReadWrite.scala:445), which has no missing parents
2026-01-16 15:48:57.531 | 26/01/16 21:48:57 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 238.4 KiB, free 412.5 MiB)
2026-01-16 15:48:57.532 | 26/01/16 21:48:57 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 87.0 KiB, free 412.4 MiB)
2026-01-16 15:48:57.533 | 26/01/16 21:48:57 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:48:57.533 | 26/01/16 21:48:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[74] at text at ReadWrite.scala:445) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:48:57.533 | 26/01/16 21:48:57 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0
2026-01-16 15:48:57.533 | 26/01/16 21:48:57 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 19) (10.1.4.95,executor 1, partition 0, PROCESS_LOCAL, 10214 bytes) 
2026-01-16 15:48:59.730 | 26/01/16 21:48:59 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 19) in 2196 ms on 10.1.4.95 (executor 1) (1/1)
2026-01-16 15:48:59.730 | 26/01/16 21:48:59 INFO TaskSchedulerImpl: Removed TaskSet 29.0 whose tasks have all completed, from pool 
2026-01-16 15:48:59.730 | 26/01/16 21:48:59 INFO DAGScheduler: ResultStage 29 (text at ReadWrite.scala:445) finished in 2208 ms
2026-01-16 15:48:59.735 | 26/01/16 21:48:59 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
2026-01-16 15:48:59.735 | 26/01/16 21:48:59 INFO TaskSchedulerImpl: Canceling stage 29
2026-01-16 15:48:59.735 | 26/01/16 21:48:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
2026-01-16 15:48:59.735 | 26/01/16 21:48:59 INFO DAGScheduler: Job 19 finished: text at ReadWrite.scala:445, took 2212.784534 ms
2026-01-16 15:48:59.735 | 26/01/16 21:48:59 INFO FileFormatWriter: Start to commit write Job bc842b15-98db-46e4-b888-f3a69ed055ed.
2026-01-16 15:48:59.735 | 26/01/16 21:48:59 INFO AbstractS3ACommitter: Starting: Task committer attempt_202601162148565414995386474839373_0000_m_000000_0: commitJob((no job ID))
2026-01-16 15:48:59.864 | 26/01/16 21:48:59 INFO AbstractS3ACommitter: Starting: committing the output of 1 task(s)
2026-01-16 15:48:59.865 | 26/01/16 21:48:59 INFO AbstractS3ACommitter: Starting: Loading and committing files in pendingset s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/4_StandardScaler_64a27f836f1c/metadata/__magic_job-bc842b15-98db-46e4-b888-f3a69ed055ed/job-bc842b15-98db-46e4-b888-f3a69ed055ed/00/task_202601162148573202191049428154121_0029_m_000000.pendingset
2026-01-16 15:48:59.997 | 26/01/16 21:48:59 INFO AbstractS3ACommitter: Loading and committing files in pendingset s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/4_StandardScaler_64a27f836f1c/metadata/__magic_job-bc842b15-98db-46e4-b888-f3a69ed055ed/job-bc842b15-98db-46e4-b888-f3a69ed055ed/00/task_202601162148573202191049428154121_0029_m_000000.pendingset: duration 0:00.132s
2026-01-16 15:48:59.998 | 26/01/16 21:48:59 INFO AbstractS3ACommitter: committing the output of 1 task(s): duration 0:00.135s
2026-01-16 15:48:59.998 | 26/01/16 21:48:59 INFO CommitOperations: Starting: Writing success file s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/4_StandardScaler_64a27f836f1c/metadata/_SUCCESS
2026-01-16 15:49:00.238 | 26/01/16 21:49:00 INFO CommitOperations: Writing success file s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/4_StandardScaler_64a27f836f1c/metadata/_SUCCESS: duration 0:00.239s
2026-01-16 15:49:00.238 | 26/01/16 21:49:00 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
2026-01-16 15:49:00.238 | 26/01/16 21:49:00 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/4_StandardScaler_64a27f836f1c/metadata
2026-01-16 15:49:00.353 | 26/01/16 21:49:00 INFO AbstractS3ACommitter: No pending uploads were found
2026-01-16 15:49:00.353 | 26/01/16 21:49:00 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/4_StandardScaler_64a27f836f1c/metadata: duration 0:00.115s
2026-01-16 15:49:00.353 | 26/01/16 21:49:00 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.115s
2026-01-16 15:49:00.353 | 26/01/16 21:49:00 INFO MagicS3GuardCommitter: Starting: Deleting magic directory s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/4_StandardScaler_64a27f836f1c/metadata/__magic_job-bc842b15-98db-46e4-b888-f3a69ed055ed
2026-01-16 15:49:01.397 | 26/01/16 21:49:01 INFO MagicS3GuardCommitter: Deleting magic directory s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model/stages/4_StandardScaler_64a27f836f1c/metadata/__magic_job-bc842b15-98db-46e4-b888-f3a69ed055ed: duration 0:01.043s
2026-01-16 15:49:01.397 | 26/01/16 21:49:01 INFO AbstractS3ACommitter: Task committer attempt_202601162148565414995386474839373_0000_m_000000_0: commitJob((no job ID)): duration 0:01.662s
2026-01-16 15:49:01.624 | 26/01/16 21:49:01 INFO FileFormatWriter: Write Job bc842b15-98db-46e4-b888-f3a69ed055ed committed. Elapsed time: 1889 ms.
2026-01-16 15:49:01.625 | 26/01/16 21:49:01 INFO FileFormatWriter: Finished processing stats for write job bc842b15-98db-46e4-b888-f3a69ed055ed.
2026-01-16 15:49:01.890 | 26/01/16 21:49:01 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2026-01-16 15:49:01.891 | 26/01/16 21:49:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
2026-01-16 15:49:01.891 | 26/01/16 21:49:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2026-01-16 15:49:01.892 | 26/01/16 21:49:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2026-01-16 15:49:01.892 | 26/01/16 21:49:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
2026-01-16 15:49:01.892 | 26/01/16 21:49:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2026-01-16 15:49:01.892 | 26/01/16 21:49:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2026-01-16 15:49:02.844 | 26/01/16 21:49:02 INFO CodeGenerator: Code generated in 4.796176 ms
2026-01-16 15:49:02.844 | 26/01/16 21:49:02 INFO SparkContext: Starting job: parquet at StandardScaler.scala:214
2026-01-16 15:49:02.844 | 26/01/16 21:49:02 INFO DAGScheduler: Got job 20 (parquet at StandardScaler.scala:214) with 1 output partitions
2026-01-16 15:49:02.844 | 26/01/16 21:49:02 INFO DAGScheduler: Final stage: ResultStage 30 (parquet at StandardScaler.scala:214)
2026-01-16 15:49:02.844 | 26/01/16 21:49:02 INFO DAGScheduler: Parents of final stage: List()
2026-01-16 15:49:02.844 | 26/01/16 21:49:02 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:49:02.844 | 26/01/16 21:49:02 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[77] at parquet at StandardScaler.scala:214), which has no missing parents
2026-01-16 15:49:02.852 | 26/01/16 21:49:02 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 239.9 KiB, free 412.8 MiB)
2026-01-16 15:49:02.854 | 26/01/16 21:49:02 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 87.3 KiB, free 412.7 MiB)
2026-01-16 15:49:02.855 | 26/01/16 21:49:02 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:49:02.855 | 26/01/16 21:49:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[77] at parquet at StandardScaler.scala:214) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:49:02.856 | 26/01/16 21:49:02 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
2026-01-16 15:49:02.857 | 26/01/16 21:49:02 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 20) (10.1.4.95,executor 1, partition 0, PROCESS_LOCAL, 10206 bytes) 
2026-01-16 15:49:05.575 | 26/01/16 21:49:05 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 20) in 2718 ms on 10.1.4.95 (executor 1) (1/1)
2026-01-16 15:49:05.575 | 26/01/16 21:49:05 INFO TaskSchedulerImpl: Removed TaskSet 30.0 whose tasks have all completed, from pool 
2026-01-16 15:49:05.575 | 26/01/16 21:49:05 INFO DAGScheduler: ResultStage 30 (parquet at StandardScaler.scala:214) finished in 2731 ms
2026-01-16 15:49:05.575 | 26/01/16 21:49:05 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
2026-01-16 15:49:05.575 | 26/01/16 21:49:05 INFO TaskSchedulerImpl: Canceling stage 30
2026-01-16 15:49:05.575 | 26/01/16 21:49:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
2026-01-16 15:49:05.575 | 26/01/16 21:49:05 INFO DAGScheduler: Job 20 finished: parquet at StandardScaler.scala:214, took 2734.659443 ms
2026-01-16 15:49:05.575 | 26/01/16 21:49:05 INFO FileFormatWriter: Start to commit write Job e74fa6e7-7b05-4473-8175-69a27745a19f.
2026-01-16 15:49:08.811 | 26/01/16 21:49:08 INFO FileFormatWriter: Write Job e74fa6e7-7b05-4473-8175-69a27745a19f committed. Elapsed time: 3234 ms.
2026-01-16 15:49:08.811 | 26/01/16 21:49:08 INFO FileFormatWriter: Finished processing stats for write job e74fa6e7-7b05-4473-8175-69a27745a19f.
2026-01-16 15:49:08.811 | 26/01/16 21:49:08 INFO Instrumentation: [c9f526fe] training finished
2026-01-16 15:49:08.811 | 26/01/16 21:49:08 INFO Instrumentation: [19ad5730] training finished
2026-01-16 15:49:08.813 | 2026-01-16 21:49:08,811 - SparkPreprocessing - INFO - PipelineModel saved to s3a://k8s-mlops-platform-bucket/v1/artifacts/pipeline_model
2026-01-16 15:49:08.813 | 2026-01-16 21:49:08,811 - SparkPreprocessing - INFO - Writing data | write_batch_size=100000
2026-01-16 15:49:08.844 | 26/01/16 21:49:08 INFO FileSourceStrategy: Pushed Filters: 
2026-01-16 15:49:08.844 | 26/01/16 21:49:08 INFO FileSourceStrategy: Post-Scan Filters: Set()
2026-01-16 15:49:09.795 | 26/01/16 21:49:09 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2026-01-16 15:49:09.795 | 26/01/16 21:49:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
2026-01-16 15:49:09.795 | 26/01/16 21:49:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2026-01-16 15:49:09.795 | 26/01/16 21:49:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2026-01-16 15:49:09.795 | 26/01/16 21:49:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
2026-01-16 15:49:09.795 | 26/01/16 21:49:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2026-01-16 15:49:09.795 | 26/01/16 21:49:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2026-01-16 15:49:10.801 | 26/01/16 21:49:10 INFO CodeGenerator: Code generated in 48.193156 ms
2026-01-16 15:49:10.805 | 26/01/16 21:49:10 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 237.9 KiB, free 412.8 MiB)
2026-01-16 15:49:10.814 | 26/01/16 21:49:10 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 41.4 KiB, free 413.1 MiB)
2026-01-16 15:49:10.816 | 26/01/16 21:49:10 INFO SparkContext: Created broadcast 26 from parquet at NativeMethodAccessorImpl.java:0
2026-01-16 15:49:10.817 | 26/01/16 21:49:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2026-01-16 15:49:10.824 | 26/01/16 21:49:10 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
2026-01-16 15:49:10.825 | 26/01/16 21:49:10 INFO DAGScheduler: Got job 21 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
2026-01-16 15:49:10.825 | 26/01/16 21:49:10 INFO DAGScheduler: Final stage: ResultStage 31 (parquet at NativeMethodAccessorImpl.java:0)
2026-01-16 15:49:10.825 | 26/01/16 21:49:10 INFO DAGScheduler: Parents of final stage: List()
2026-01-16 15:49:10.826 | 26/01/16 21:49:10 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:49:10.827 | 26/01/16 21:49:10 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[81] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
2026-01-16 15:49:10.846 | 26/01/16 21:49:10 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 311.9 KiB, free 412.8 MiB)
2026-01-16 15:49:10.849 | 26/01/16 21:49:10 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 110.9 KiB, free 412.7 MiB)
2026-01-16 15:49:10.850 | 26/01/16 21:49:10 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:49:10.850 | 26/01/16 21:49:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[81] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:49:10.850 | 26/01/16 21:49:10 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0
2026-01-16 15:49:10.852 | 26/01/16 21:49:10 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 21) (10.1.4.95,executor 1, partition 0, PROCESS_LOCAL, 10244 bytes) 
2026-01-16 15:49:15.837 | 26/01/16 21:49:15 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 21) in 4986 ms on 10.1.4.95 (executor 1) (1/1)
2026-01-16 15:49:15.837 | 26/01/16 21:49:15 INFO TaskSchedulerImpl: Removed TaskSet 31.0 whose tasks have all completed, from pool 
2026-01-16 15:49:15.837 | 26/01/16 21:49:15 INFO DAGScheduler: ResultStage 31 (parquet at NativeMethodAccessorImpl.java:0) finished in 5010 ms
2026-01-16 15:49:15.837 | 26/01/16 21:49:15 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
2026-01-16 15:49:15.837 | 26/01/16 21:49:15 INFO TaskSchedulerImpl: Canceling stage 31
2026-01-16 15:49:15.837 | 26/01/16 21:49:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished
2026-01-16 15:49:15.838 | 26/01/16 21:49:15 INFO DAGScheduler: Job 21 finished: parquet at NativeMethodAccessorImpl.java:0, took 5013.018085 ms
2026-01-16 15:49:15.838 | 26/01/16 21:49:15 INFO FileFormatWriter: Start to commit write Job 23883a3f-37c9-43d1-a7be-a3b7e50cde34.
2026-01-16 15:49:18.999 | 26/01/16 21:49:18 INFO FileFormatWriter: Write Job 23883a3f-37c9-43d1-a7be-a3b7e50cde34 committed. Elapsed time: 3160 ms.
2026-01-16 15:49:18.999 | 26/01/16 21:49:18 INFO FileFormatWriter: Finished processing stats for write job 23883a3f-37c9-43d1-a7be-a3b7e50cde34.
2026-01-16 15:49:18.999 | 2026-01-16 21:49:18,999 - SparkPreprocessing - INFO - Data written successfully to s3a://k8s-mlops-platform-bucket/v1/processed/train/
2026-01-16 15:49:18.999 | 2026-01-16 21:49:18,999 - SparkPreprocessing - INFO - Loading feature pipeline: preprocessing.preprocessing_001
2026-01-16 15:49:18.999 | 2026-01-16 21:49:18,999 - SparkPreprocessing - INFO - Loading data from s3a://k8s-mlops-platform-bucket/v1/raw/val/
2026-01-16 15:49:19.713 | 26/01/16 21:49:19 INFO HadoopFSUtils: Listing s3a://k8s-mlops-platform-bucket/v1/raw/val with listFiles API
2026-01-16 15:49:19.833 | 26/01/16 21:49:19 INFO InMemoryFileIndex: It took 120 ms to list leaf files for 1 paths.
2026-01-16 15:49:19.836 | 26/01/16 21:49:19 INFO FileSourceStrategy: Pushed Filters: 
2026-01-16 15:49:19.836 | 26/01/16 21:49:19 INFO FileSourceStrategy: Post-Scan Filters: Set()
2026-01-16 15:49:19.841 | 26/01/16 21:49:19 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 237.9 KiB, free 412.5 MiB)
2026-01-16 15:49:19.847 | 26/01/16 21:49:19 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 41.4 KiB, free 412.4 MiB)
2026-01-16 15:49:19.848 | 26/01/16 21:49:19 INFO SparkContext: Created broadcast 28 from javaToPython at NativeMethodAccessorImpl.java:0
2026-01-16 15:49:19.849 | 26/01/16 21:49:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2026-01-16 15:49:19.855 | 2026-01-16 21:49:19,854 - SparkPreprocessing - INFO - Data loaded successfully | partitions: 1
2026-01-16 15:49:20.001 | 2026-01-16 21:49:20,001 - SparkPreprocessing - INFO - Writing data | write_batch_size=100000
2026-01-16 15:49:20.030 | 26/01/16 21:49:20 INFO FileSourceStrategy: Pushed Filters: 
2026-01-16 15:49:20.030 | 26/01/16 21:49:20 INFO FileSourceStrategy: Post-Scan Filters: Set()
2026-01-16 15:49:20.268 | 26/01/16 21:49:20 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2026-01-16 15:49:20.269 | 26/01/16 21:49:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
2026-01-16 15:49:20.270 | 26/01/16 21:49:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2026-01-16 15:49:20.270 | 26/01/16 21:49:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2026-01-16 15:49:20.270 | 26/01/16 21:49:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
2026-01-16 15:49:20.270 | 26/01/16 21:49:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2026-01-16 15:49:20.270 | 26/01/16 21:49:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2026-01-16 15:49:21.269 | 26/01/16 21:49:21 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 237.9 KiB, free 412.9 MiB)
2026-01-16 15:49:21.270 | 26/01/16 21:49:21 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 41.4 KiB, free 412.8 MiB)
2026-01-16 15:49:21.273 | 26/01/16 21:49:21 INFO SparkContext: Created broadcast 29 from parquet at NativeMethodAccessorImpl.java:0
2026-01-16 15:49:21.273 | 26/01/16 21:49:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2026-01-16 15:49:21.278 | 26/01/16 21:49:21 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
2026-01-16 15:49:21.278 | 26/01/16 21:49:21 INFO DAGScheduler: Got job 22 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
2026-01-16 15:49:21.278 | 26/01/16 21:49:21 INFO DAGScheduler: Final stage: ResultStage 32 (parquet at NativeMethodAccessorImpl.java:0)
2026-01-16 15:49:21.278 | 26/01/16 21:49:21 INFO DAGScheduler: Parents of final stage: List()
2026-01-16 15:49:21.278 | 26/01/16 21:49:21 INFO DAGScheduler: Missing parents: List()
2026-01-16 15:49:21.278 | 26/01/16 21:49:21 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[91] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
2026-01-16 15:49:21.286 | 26/01/16 21:49:21 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 311.9 KiB, free 412.5 MiB)
2026-01-16 15:49:21.288 | 26/01/16 21:49:21 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 110.9 KiB, free 412.4 MiB)
2026-01-16 15:49:21.289 | 26/01/16 21:49:21 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1676
2026-01-16 15:49:21.289 | 26/01/16 21:49:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[91] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2026-01-16 15:49:21.289 | 26/01/16 21:49:21 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
2026-01-16 15:49:21.290 | 26/01/16 21:49:21 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 22) (10.1.4.95,executor 1, partition 0, PROCESS_LOCAL, 10247 bytes) 
2026-01-16 15:49:25.491 | 26/01/16 21:49:25 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 22) in 4200 ms on 10.1.4.95 (executor 1) (1/1)
2026-01-16 15:49:25.491 | 26/01/16 21:49:25 INFO TaskSchedulerImpl: Removed TaskSet 32.0 whose tasks have all completed, from pool 
2026-01-16 15:49:25.491 | 26/01/16 21:49:25 INFO DAGScheduler: ResultStage 32 (parquet at NativeMethodAccessorImpl.java:0) finished in 4215 ms
2026-01-16 15:49:25.491 | 26/01/16 21:49:25 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
2026-01-16 15:49:25.491 | 26/01/16 21:49:25 INFO TaskSchedulerImpl: Canceling stage 32
2026-01-16 15:49:25.491 | 26/01/16 21:49:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
2026-01-16 15:49:25.491 | 26/01/16 21:49:25 INFO DAGScheduler: Job 22 finished: parquet at NativeMethodAccessorImpl.java:0, took 4215.94507 ms
2026-01-16 15:49:25.491 | 26/01/16 21:49:25 INFO FileFormatWriter: Start to commit write Job f6153426-926d-46a0-9867-e1899f27d4ea.
2026-01-16 15:49:29.283 | 26/01/16 21:49:29 INFO FileFormatWriter: Write Job f6153426-926d-46a0-9867-e1899f27d4ea committed. Elapsed time: 3644 ms.
2026-01-16 15:49:29.284 | 26/01/16 21:49:29 INFO FileFormatWriter: Finished processing stats for write job f6153426-926d-46a0-9867-e1899f27d4ea.
2026-01-16 15:49:29.284 | 2026-01-16 21:49:29,284 - SparkPreprocessing - INFO - Data written successfully to s3a://k8s-mlops-platform-bucket/v1/processed/val/
2026-01-16 15:49:29.284 | 2026-01-16 21:49:29,284 - SparkPreprocessing - INFO - Loading feature pipeline: preprocessing.preprocessing_001
2026-01-16 15:49:29.284 | 2026-01-16 21:49:29,284 - SparkPreprocessing - INFO - Loading data from s3a://k8s-mlops-platform-bucket/v1/raw/test/
2026-01-16 15:49:29.993 | 26/01/16 21:49:29 INFO HadoopFSUtils: Listing s3a://k8s-mlops-platform-bucket/v1/raw/test with listFiles API
2026-01-16 15:49:30.119 | 26/01/16 21:49:30 INFO InMemoryFileIndex: It took 127 ms to list leaf files for 1 paths.
2026-01-16 15:49:30.123 | 26/01/16 21:49:30 INFO FileSourceStrategy: Pushed Filters: 
2026-01-16 15:49:30.123 | 26/01/16 21:49:30 INFO FileSourceStrategy: Post-Scan Filters: Set()
2026-01-16 15:49:30.126 | 26/01/16 21:49:30 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 237.9 KiB, free 412.2 MiB)
2026-01-16 15:49:30.136 | 26/01/16 21:49:30 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 41.4 KiB, free 412.1 MiB)
2026-01-16 15:49:30.137 | 26/01/16 21:49:30 INFO SparkContext: Created broadcast 31 from javaToPython at NativeMethodAccessorImpl.java:0
2026-01-16 15:49:30.138 | 26/01/16 21:49:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2026-01-16 15:49:30.144 | 2026-01-16 21:49:30,143 - SparkPreprocessing - INFO - Data loaded successfully | partitions: 1
2026-01-16 15:49:30.294 | 2026-01-16 21:49:30,294 - SparkPreprocessing - INFO - Writing data | write_batch_size=100000
2026-01-16 15:49:30.312 | 26/01/16 21:49:30 INFO FileSourceStrategy: Pushed Filters: 
2026-01-16 15:49:30.312 | 26/01/16 21:49:30 INFO FileSourceStrategy: Post-Scan Filters: Set()