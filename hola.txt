Eres un agente experto en Python, Machine Learning, Ray Train y Ray Tune. Tu tarea es generar un proyecto modular de Machine Learning llamado "CubeRay" que cumpla con los siguientes requisitos:

1. **Generalidad y modularidad:**
   - El proyecto debe tener un `main.py` que sirva como punto de entrada general, capaz de entrenar cualquier modelo y cualquier dataset.
   - Los modelos deben ser intercambiables y seleccionables desde un archivo de configuración tipo `params.yaml`.
   - No debe haber código duplicado en `main.py`; todo el entrenamiento y hyperparameter tuning debe estar en módulos independientes.

2. **Frameworks iniciales:**
   - Debe soportar **XGBoost** y **PyTorch** desde el inicio.
   - Cada framework debe tener:
     - Una función de entrenamiento (`train_model`) usando **Ray Train**.
     - Una función de hyperparameter tuning (`tune_model`) usando **Ray Tune**.
     - Todos los parámetros posibles de cada modelo en un **diccionario Python** dentro de un módulo `.py` separado, no Pydantic.  
       - Ejemplo: `schemas/xgboost_params.py` contiene `XGBOOST_PARAMS = {...}` con todos los parámetros posibles de XGBoost.
       - Ejemplo: `schemas/pytorch_params.py` contiene `PYTORCH_PARAMS = {...}` con todos los parámetros posibles de PyTorch.
     - Para PyTorch, incluye una clase de red neuronal de ejemplo en el módulo correspondiente.

3. **Configuración y parámetros:**
   - `params.yaml` define:
     - Qué modelo usar (`framework: xgboost` o `framework: pytorch`).
     - Parámetros generales como número de épocas, batch size, etc.
   - `main.py` debe importar **dinámicamente** el módulo de parámetros y funciones según lo seleccionado en `params.yaml`.  
     - Por ejemplo, si `framework: xgboost`, importa `xgboost_module.py`; si `framework: pytorch`, importa `pytorch_module.py`.
   - Solo un `try/except` por función; no múltiples capturas dispersas.

4. **MLflow:**
   - Incluir integración mínima de MLflow para tracking de parámetros, métricas y artefactos.
   - El proyecto debe estar preparado para guardar experimentos y resultados con MLflow, aunque el usuario pueda decidir dónde apuntar el tracking server y el artifact store.

5. **Documentación oficial (para que sigas referencias y mejores prácticas):**
   - Ray Train: https://docs.ray.io/en/latest/train/overview.html
   - Ray Tune: https://docs.ray.io/en/latest/tune/getting-started.html
   - Ray Train Examples:
     - XGBoost: https://docs.ray.io/en/latest/train/getting-started-xgboost.html
     - PyTorch Lightning: https://docs.ray.io/en/latest/train/getting-started-pytorch-lightning.html
   - Ray Tune Examples:
     - XGBoost: https://docs.ray.io/en/latest/tune/examples/tune-xgboost.html
     - PyTorch Lightning: https://docs.ray.io/en/latest/tune/examples/tune-pytorch-lightning.html
   - Tune + MLflow: https://docs.ray.io/en/latest/tune/examples/includes/mlflow_ptl_example.html
   - **Nota:** Asume que sabes poco de Ray y debe basarse principalmente en la documentación oficial en lugar de conocimientos previos.

6. **Arquitectura de archivos sugerida (dentro de la carpeta kubeRay):**

k3s/                # carpeta padre
├─ kuberay/
|  ├─ main.py
|  ├─ utils.py
├─ params.yaml
   ├─ modules/
   │   ├─ __init__.py
   │   ├─ xgboost_module.py
   │   └─ pytorch_module.py
   ├─ schemas/
   │   ├─ __init__.py
   │   ├─ xgboost_params.py
   │   └─ pytorch_params.py
   └─ utils/
       ├─ __init__.py
       └─ ray_utils.py  # Funciones auxiliares para Ray Train y Tune

7. **Requisitos de implementación:**
   - El main.py debe:
     - Leer `params.yaml`.
     - Cargar los parámetros correspondientes desde los diccionarios en `schemas/`.
     - Ejecutar entrenamiento y tuning según la configuración.
     - Funcionar para cualquier dataset genérico (X_train, y_train, X_test, y_test).
   - Implementar MLflow para tracking de experimentos y artefactos de manera clara y modular.
   - Mantener código limpio, modular y legible, siguiendo las mejores prácticas de Ray Train, Ray Tune y MLflow.

8. **Extras opcionales:**
   - Guardar modelos y logs en S3 o MinIO.
   - Soporte para checkpoints y early stopping si aplica.
   - El uri de mlflow y el nombre del experimento estaria en params.yaml

9. **Proceso de desarrollo sugerido (en rondas):**
   - **Ronda 1:** Revisar la documentación oficial proporcionada y extraer cómo implementar cada función, las mejores prácticas y ejemplos válidos de XGBoost y PyTorch.
   - **Ronda 2:** Aplicar las mejores prácticas encontradas a la implementación del proyecto kubeRay.
   - **Ronda 3:** Revisar los modelos generados, checar el código y validar que todo funcione correctamente.
   - **Ronda 4:** Tomar como referencia el código que ya existe en la carpeta `preprocessing/preprocessing_001.py` del proyecto Spark, especialmente la estructura del main.py y las funciones auxiliares, para asegurarse de seguir un estilo de programación legible y consistente.
   - **Ronda 5:** Revisar y adaptar la legibilidad, consistencia y estilo de programación para que coincida con el estilo que se usa en `main.py` de kuberay y spark.