# RayService -> La forma correcta de hacer inferencia en KubeRay.
# Gestiona el ciclo de vida del RayCluster y el RayServe application.
apiVersion: ray.io/v1
kind: RayService
metadata:
  name: inference-service
  namespace: ray
spec:
  serviceUnhealthySecondThreshold: 900
  deploymentUnhealthySecondThreshold: 900

  # Configuración de la aplicación Serve
  serveConfigV2: |
    applications:
      - name: inference_app
        import_path: inference.main:app
        route_prefix: /inference
        runtime_env:
          env_vars:
            FRAMEWORK: "xgboost"
            MODEL_BUCKET: "mlops-platform"
            MODEL_KEY: "models/model_xgboost.pkl"
            ARTIFACTS_KEY: "processed/pipeline_model.json"
            # Credenciales se inyectan via secret abajo

  # Definición del Cluster (Inspirado en kuberay-job.yaml)
  rayClusterConfig:
    rayVersion: '2.52.0'
    headGroupSpec:
      rayStartParams:
        dashboard-host: '0.0.0.0'
      template:
        spec:
          containers:
          - name: ray-head
            image: rayproject/ray:2.52.0
            resources:
              limits:
                cpu: "1"
                memory: "4Gi"
              requests:
                cpu: "1"
                memory: "2Gi"
            ports:
            - containerPort: 6379
              name: gcs-server
            - containerPort: 8265
              name: dashboard
            - containerPort: 10001
              name: client
            - containerPort: 8000
              name: serve
            volumeMounts:
              - name: job-files
                mountPath: /app
            envFrom:
              - secretRef:
                  name: env-secret
          initContainers:
          - name: git-sync
            image: registry.k8s.io/git-sync/git-sync:v4.1.0
            resources:
              requests:
                cpu: 100m
                memory: 128Mi
            env:
              - name: GITSYNC_REPO
                value: "https://github.com/tu-usuario/Kubernetes-MLOPS-platform" # Reemplazar con tu repo real o env var
              - name: GITSYNC_ROOT
                value: "/app"
              - name: GITSYNC_DEST
                value: "repo"
            volumeMounts:
              - name: job-files
                mountPath: /app
          volumes:
            - name: job-files
              emptyDir: {}
    
    workerGroupSpecs:
      - replicas: 1
        minReplicas: 0
        maxReplicas: 2
        groupName: inference-worker
        rayStartParams: {}
        template:
          spec:
            containers:
              - name: ray-worker
                image: rayproject/ray:2.52.0
                resources:
                  limits:
                    cpu: "1"
                    memory: "2Gi"
                  requests:
                    cpu: "1"
                    memory: "2Gi"
                envFrom:
                  - secretRef:
                      name: env-secret
                volumeMounts: # Worker needs code too for custom modules
                  - name: job-files
                    mountPath: /app 
            initContainers:
              - name: git-sync
                image: registry.k8s.io/git-sync/git-sync:v4.1.0
                env:
                  - name: GITSYNC_REPO
                    value: "https://github.com/tu-usuario/Kubernetes-MLOPS-platform"
                  - name: GITSYNC_ROOT
                    value: "/app"
                  - name: GITSYNC_DEST
                    value: "repo"
                volumeMounts:
                  - name: job-files
                    mountPath: /app
            volumes:
              - name: job-files
                emptyDir: {}

---
# Service LoadBalancer para exponer Ray Serve
# Apuntamos al servicio estable que el operador crea automáticamente
apiVersion: v1
kind: Service
metadata:
  name: inference-external
  namespace: ray
spec:
  type: LoadBalancer
  ports:
  - name: http
    port: 80
    targetPort: 8000 # Puerto de Ray Serve
  selector:
    # Este label lo añade el operador RayService a todos sus pods
    ray.io/service: inference-service 
    ray.io/node-type: head

